= Implementing the DAG in Jet's Core API

Now that we've come up with a good DAG design, we can use Jet's Core API
to implement it. We start by instantiating the DAG class and adding the
source vertex:

[source]
DAG dag = new DAG();
Vertex source = dag.newVertex("source", SourceProcessors.readMapP("lines"));

Note how we can build the DAG outside the context of any running Jet
instances: it is a pure POJO.

The source vertex will read the lines from the `IMap` and emit items of
type `Map.Entry<Integer, String>` to the next vertex. The key of the
entry is the line number, and the value is the line itself. The built-in
map-reading processor will do just what we want: on each member it will
read only the data local to that member.

The next vertex is the _tokenizer_, which does a simple "flat-mapping"
operation (transforms one input item into zero or more output items).
The low-level support for such a processor is a part of Jet's library,
we just need to provide the mapping function:

[source]
// (lineNum, line) -> words
Pattern delimiter = Pattern.compile("\\W+");
Vertex tokenize = dag.newVertex("tokenize",
    Processors.flatMapP((Entry<Integer, String> e) ->
        traverseArray(delimiter.split(e.getValue().toLowerCase()))
              .filter(word -> !word.isEmpty()))
);

This creates a processor that applies the given function to each
incoming item, obtaining zero or more output items, and emits them.
Specifically, our processor accepts items of type `Entry<Integer,
String>`, splits the entry value into lowercase words, and emits all
non-empty words. The function must return a `Traverser`, which is a
functional interface used to traverse a sequence of non-null items. Its
purpose is equivalent to the standard Java `Iterator`, but avoids the
cumbersome two-method API. Since a lot of support for cooperative
multithreading in Hazelcast Jet deals with sequence traversal, this
abstraction simplifies many of its aspects.

The next vertex will do the actual word count. We can use the built-in
`accumulateByKey` processor for this:

[source]
// word -> (word, count)
Vertex accumulate = dag.newVertex("accumulate",
        Processors.accumulateByKeyP(wholeItem(), counting())
);

This processor maintains a hashtable that maps each distinct key to its
accumulated value. We specify `wholeItem()` as the _key extractor_
function: our input item is just the word, which is also the grouping
key. The second argument is the kind of aggregate operation we want to
perform: counting. We are relying on Jet's out-of-the-box
definitions here, but it is easy to define your own aggregate operations
and key extractors. The processor emits nothing until it has received
all the input, and at that point it emits the hashtable as a stream of
`Entry<String, Long>`.

Next is the combining step which computes the grand totals from
individual members' contributions. This is the code:

[source]
// (word, count) -> (word, count)
Vertex combine = dag.newVertex("combine",
    Processors.combineByKeyP(counting())
);

`combineByKey` is designed to be used downstream of `accumulateByKey`,
which is why it doesn't need an explicit key extractor. The aggregate
operation must be the same as on `accumulateByKey`.

The final vertex is the sink; we want to store the output in
another `IMap`:

[source]
Vertex sink = dag.newVertex("sink", SinkProcessors.writeMapP("counts"));

Now that we have all the vertices, we must connect them into a graph and
specify the edge type as discussed in the previous section. Here's all
the code at once:

[source]
dag.edge(between(source, tokenize))
   .edge(between(tokenize, accumulate)
           .partitioned(wholeItem(), Partitioner.HASH_CODE))
   .edge(between(accumulate, combine)
           .distributed()
           .partitioned(entryKey()))
   .edge(between(combine, sink));

Let's take a closer look at some of the edges. First, source to
tokenizer:

[source]
 .edge(between(tokenize, accumulate)
       .partitioned(wholeItem(), Partitioner.HASH_CODE))

We chose a _local partitioned_ edge. For each word, there will be a
processor responsible for it on each member so that no items must travel
across the network. In the `partitioned()` call we specify two things:
the function that extracts the partitioning key (`wholeItem()` - same as
the grouping key extractor), and the policy object that decides
how to compute the partition ID from the key. Here we use the built-in
`HASH_CODE`, which will derive the ID from `Object.hashCode()`. As long
as the the definitions of `equals()/hashCode()` on the key object match
our expected notion of key equality, this policy is always safe to use
on a local edge.

Next, the edge from the accumulator to the combiner:

[source]
.edge(between(accumulate, combine)
       .distributed()
       .partitioned(entryKey()))

It is _distributed partitioned_: for each word there is a single
`combiner` processor in the whole cluster responsible for it and items
will be sent over the network if needed. The partitioning key is again
the word, but here it is the key part of the `Map.Entry<String, Long>`.
We are using the default partitioning policy here (Hazelcast's own
partitioning scheme). It is the slower-but-safe choice on a distributed
edge. Detailed inspection shows that hashcode-based partitioning would
be safe as well because all of `String`, `Long`, and `Map.Entry` have
the hash function specified in their Javadoc.

You can acces a full, self-contained Java program with the above DAG code at the
{jet-samples}/refman/src/main/java/refman/WordCountCoreApiRefMan.java[Hazelcast Jet code samples repository].
