= The Shape of a Pipeline

The general shape of any data processing pipeline is `drawFromSource ->
transform -> drainToSink` and the natural way to build it is from source
to sink. The {jet-javadoc}/Pipeline.html[Pipeline] API follows this
pattern. For example,

[source]
----
include::{javasource}/BuildComputation.java[tag=s1]
----

In each step, such as `drawFrom` or `drainTo`, you create a pipeline
_stage_. The stage resulting from a `drainTo` operation is called a
_sink stage_ and you can't attach more stages to it. All others are
called _compute stages_ and expect you to attach stages to them.

The API differentiates between batch (bounded) and stream (unbounded)
sources and this is reflected in the naming: there is a `BatchStage`
and a `StreamStage`, each offering the operations appropriate to its
kind. In this section we'll mostly use batch stages, for simplicity,
but the API of operations common to both kinds is identical. We'll
explain later on how to apply windowing, which is necessary to aggregate
over unbounded streams.

Your pipeline can consist of multiple sources, each starting its own
pipeline branch, and you are allowed to mix both kinds of stages in the
same pipeline. You can merge the branches with joining transforms. For
example, the <<hash-join, hash-join>> transform can join a stream stage
with batch stages:

[source]
----
include::{javasource}/BuildComputation.java[tag=s2]
----

Symmetrically, you can fork the output of a stage and send it to more
than one destination:

[source]
----
include::{javasource}/BuildComputation.java[tag=s3]
----

= Choose Your Data Sources and Sinks

Hazelcast Jet has support for these data sources and sinks:

- Hazelcast `IMap` and `ICache`, both as a batch source of just their
contents and their event journal as an infinite source
- Hazelcast `IList` (batch)
- Hadoop Distributed File System (HDFS) (batch)
- Java Database Connectivity (JDBC) (batch)
- Java Messaging Services (JMS) queue and topic (infinite stream)
- Kafka topic (infinite stream)
- TCP socket (infinite stream)
- a directory on the filesystem, both as a batch source of the current
  file contents and an infinite source of append events to the files
- Apache Avro files (batch)
- any source/sink you create on your own by using the
{jet-javadoc}/pipeline/SourceBuilder.html[`SourceBuilder`] and the
{jet-javadoc}/pipeline/SinkBuilder.html[`SinkBuilder`].

You can access most of these via the
{jet-javadoc}/pipeline/Sources.html[`Sources`] and
{jet-javadoc}/pipeline/Sinks.html[`Sinks`] utility classes.
{jet-javadoc}/kafka/KafkaSources.html[Kafka], {jet-javadoc}/hadoop/HdfsSources.html[HDFS] and
{jet-javadoc}/avro/AvroSources.html[Avro] connectors are in their separate
modules. The {jet-javadoc}/pipeline/SourceBuilder.html#batch-java.lang.String-com.hazelcast.jet.function.DistributedFunction-[source]
and {jet-javadoc}/pipeline/SinkBuilder.html#sinkBuilder-java.lang.String-com.hazelcast.jet.function.DistributedFunction-[sink]
builder factories are in their respective classes.

There's a <<source-sink-connectors, dedicated chapter>> that discusses
the topic of data sources and sinks in more detail.

= Basic Transforms: map, filter, flatMap

The simplest kind of transformation is one that can be done on each item
individually and independent of other items. The major examples are
{jet-javadoc}/pipeline/BatchStage.html#map-com.hazelcast.jet.function.DistributedFunction-[`map`],
{jet-javadoc}/pipeline/BatchStage.html#filter-com.hazelcast.jet.function.DistributedPredicate-[`filter`]
and
{jet-javadoc}/pipeline/BatchStage.html#flatMap-com.hazelcast.jet.function.DistributedFunction-[`flatMap`].
We already saw them in use in the previous examples. `map` transforms
each item to another item; `filter` discards items that don't match its
predicate; and `flatMap` transforms each item into zero or more output
items. You can refer to their Javadoc for finer detail and here we'll
move on to other kinds of transformations.

[[distinct]]
= Suppress Duplicates

The `distinct` operation suppresses the duplicates from a stream. If you
perform it after adding a grouping key, it emits a single item for every
distinct key. The operation works on both batch and stream stages. In
the latter case it emits distinct items within a window. Two different
windows are processed independently.

In this example we have a batch of `Person` objects and we choose an
arbitrary one from each 5-year age bracket:

[source]
----
include::{javasource}/BuildComputation.java[tag=s17]
----

[[merge]]
= Merge Streams

The `merge` operation combines two pipeline stages in the simplest
manner: it just emits all the items from both stages. In this example
we merge the trading events from the New York and Tokyo stock exchanges:

[source]
----
include::{javasource}/BuildComputation.java[tag=s18]
----

= Enrich Your Stream

As the data comes in, before you perform any reasoning on it, you must
look up and attach to each item all the knowledge you have about it. If
the data item represents a trade event, you want the data on the
valuable being traded, the buyer, seller, etc. We call this step _data
enrichment_.

You've got two basic techniques to enrich your data stream:

. <<hash-join, Hash-join>> the primary data stream with one or more
enriching datasets. If your enriching dataset doesn't change for the
duration of the Jet job, this is your best tool.

. <<enrich-by-lookup, Directly look up>> the data from its storage
system. If you're enriching an infinite stream and want to observe
updates to the enriching dataset, you should use the this approach.

[[hash-join]]
= Hash-Join

{jet-javadoc}/pipeline/BatchStage.html#hashJoin-com.hazelcast.jet.pipeline.BatchStage-com.hazelcast.jet.pipeline.JoinClause-com.hazelcast.jet.function.DistributedBiFunction-[Hash-join]
is a kind of join operation optimized for the use case of _data
enrichment_. It is like a many-to-one SQL JOIN that matches a foreign
key in the stream-to-be-enriched with the primary key in the enriching
dataset. You can perform several such joins in one operation, enriching
your stream from arbitrarily many sources.

The stream-to-be-enriched (we'll call in the _primary_ stream for short)
can be an unbounded data stream. The _enriching streams_ must be bounded
and Jet will consume them in full before starting to enrich the primary
stream. It will store their contents in hashtables for fast lookup,
which is why we call this the "hash-join".

For each enriching stream you specify a pair of key-extracting functions:
one for the enriching item and one for the primary item. This means that
you can define a different join key for each of the enriching streams.
The following example shows a three-way hash-join between the primary
stream of stock trade events and two enriching streams: _products_ and
_brokers_:

[source]
----
include::{javasource}/BuildComputation.java[tag=s10]
----

Products are joined on `Trade.productId` and brokers on `Trade.brokerId`.
`joinMapEntries()` returns a `JoinClause`, which is a holder of the
three functions that specify how to perform a join:

1. the key extractor for the primary stream's item
2. the key extractor for the enriching stream's item
3. the projection function that transforms the enriching stream's item
into the item that will be used for enrichment.

Typically the enriching streams will be `Map.Entry` s coming from a
key-value store, but you want just the entry value to appear as the
enriching item. In that case you'll specify `Map.Entry::getValue` as the
projection function. This is what `joinMapEntries()` does for you. It
takes just one function, primary stream's key extractor, and fills in
`Entry::getKey` and `Entry::getValue` for the enriching stream key
extractor and the projection function, respectively.

In the interest of performance Jet pulls the entire enriching dataset
into each cluster member. That's why this operation is also known as a
_replicated_ join. This is something to keep in mind when estimating
the RAM requirements for a hash-join operation.

== Hash-Join With Four or More Streams Using the Builder

You can hash-join a stream with up to two enriching streams using the
API we demonstrated above. If you have more than two enriching streams,
you'll use the
{jet-javadoc}/pipeline/StreamStage.html#hashJoinBuilder--[hash-join builder].
For example, you may want to enrich a trade with its associated product,
broker, and market:

[source]
----
include::{javasource}/BuildComputation.java[tag=s11]
----

The data type on the hash-joined stage is `Tuple2<Trade, ItemsByTag>`.
The next snippet shows how to use it to access the primary and enriching
items:

[source]
----
include::{javasource}/BuildComputation.java[tag=s12]
----

[[enrich-by-lookup]]
= Enrich Using Direct Lookup

If you're enriching an infinite stream, you most likely need to observe
the changes that happen to the enriching dataset over the long timespan
of the Jet job. In this case you can't use the hash-join, which
basically takes a snapshot of the enriching dataset at the beginning of
the job.

The `xUsingY` transforms (such as <<map-using-imap, `mapUsingIMap`>> or
<<map-using-context, `filterUsingContext`>>) can enrich a stream by
looking up from the original data source each time. There's direct
support for <<map-using-imap, Hazelcast maps>> and Jet <<map-using-context,
exposes the underlying machinery>> as well so you can write your own
code to join with an arbitrary external dataset.

[[map-using-imap]]
== Look Up from Hazelcast Map

Hazelcast Jet allows you to
enrich your stream directly from a Hazelcast `IMap` or `ReplicatedMap`.
Since it must look up the data again for each item, performance is lower
than with a <<hash-join, hash-join>>, but the data is kept fresh this way. This matters
especially for unbounded streaming jobs, where a hash-join would use
data frozen in time at the beginning of the job.

If you enrich from a Hazelcast map (`IMap` or `ReplicatedMap`) 
that is stored inside the Jet cluster, you can achieve data locality. 
For `ReplicatedMap` that's trivial because
its entire contents are present on every cluster member. `IMap`, on the
other hand, is partitioned so a given member holds only a part of the
data. You must repartition the primary stream to colocate it with the `IMap`.
Jet then sends your data item to the member that holds that particular item's 
lookup key. This still means the data travels over the
network, but it's only one way and highly streamlined. There's no
latency inherent to a request-response cycle, just the bandwidth cost of
network transfer.

In this example we enrich a stream of trades with detailed stock info.
The stock info (the enriching dataset) is stored in a Hazelcast `IMap` 
so we use `groupingKey()` to let Jet partition our stream and use local 
`IMap` lookups:

[source]
----
include::{javasource}/BuildComputation.java[tag=s16]
----

<1> Obtain the `IMap` from a Hazelcast Jet instance
<2> Set the lookup key, this enables data locality
<3> Enrich the stream by setting the looked-up `StockInfo` on `Trade`
    items. This syntax works if the setter has fluent API and returns
    `this`.

In the example above the syntax looks very simple, but it hides a layer
of complexity: you can't just fetch a Hazelcast map instance and add it
to the pipeline. It's a proxy object that's valid only in the JVM
process where you obtained it. `mapUsingIMap` actually remembers just
the name of the map and will obtain a map with the same name from the
Jet cluster where the job runs.

[[map-using-context]]
== Look Up From an External System

Hazelcast Jet exposes the facility to look up from an external system.

In this case you must define a factory object that creates a client instance
and fetches the data from the remote system of record. 

You can provide a _factory_ that Jet will use to create a context object
for each `Processor` in the cluster that executes your transforming step.
You can also specify a function that extracts the key from your items.
Even though Jet won't do any lookup or grouping by that key, it will
set up the job so that all the items with the same key get paired with
the same context object. 

For example, you may be fetching data from a remote Hazelcast cluster. 
To optimize performance, you can enable the
near-cache on the client and you can save a lot of memory by specifying
the key function: since Jet executes your enriching code in parallel,
each worker has its own Hazelcast client instance, with its own
near-cache. With the key function, each near-cache will manage its own
subset of the keys:

[source]
----
include::{javasource}/BuildComputation.java[tag=s16a]
----

In a similar fashion you can integrate other external systems with a
Jet pipeline.

When you use the `xUsingY` transform to enrich an infinite stream, your
output will reflect the fact that the enriching dataset is changing. Two
items with the same key may be enriched with different data. This is
expected. However, the output may also behave in a surprising way. Say
you change the enriching data for some key from A to B. You expect to
see a sequence ...`A-A-B-B`... in the output, but when you sort it by
timestamp, you can observe any order, such as `A-B-A-B` or `B-B-A-A`.
This is because Jet doesn't enforce the processing order to strictly
follow the time order of events. Currently there is no feature in Jet
that would achieve monotonic read consistency while enriching an event
stream from a changing dataset.

= Group and Aggregate

Data aggregation is the cornerstone of distributed stream processing. It
computes an aggregate function (simple examples: sum or average) over
the data items. Typically you don't want to agregate all the items
together, but classify them by some key and then aggregate over each
group separately. This is the _group-and-aggregate_ transformation. You
can join several streams on a key and simultaneously group-aggregate
them on the same key. This is the _cogroup-and-aggregate_ transformation.
If you're processing an unbounded event stream, you must define a
bounded _window_ over the stream within which Jet will aggregate the
data.

This is how a very simple batch aggregation without grouping may look
(the list named "`text`" contains lines of text):

[source]
----
include::{javasource}/BuildComputation.java[tag=s4]
----

We simply counted all the lines and pushed the count to a list named
"`result`". Let's count all the words instead:

[source]
----
include::{javasource}/BuildComputation.java[tag=s5]
----

We split up the lines into words using a regular expression. Blank lines
result in empty "`words`" so we filter these out.

Now let's turn this into something more insigtful: a word frequency
histogram, giving us for each distinct word the number of its
occurrences:

[source]
----
include::{javasource}/BuildComputation.java[tag=s6]
----

We just added a grouping key, in this case it is the word itself
(`Util.wholeItem()` returns the identity function). Now our pipeline
produces not one, but as many distinct result items as there are words
in the text. What we've just built up is the classical Word Count task.

The definition of the aggregate operation hides behind the `counting()`
method call. This is a static method in our
{jet-javadoc}/aggregate/AggregateOperations.html[`AggregateOperations`]
utility class, which provides you with some predefined aggregate
operations. You can also implement your own aggregate operations; please
refer to the section <<implement-your-aggregate-operation,
dedicated to this>>.

[[cogroup]]
= Join Streams and Aggregate in One Step

Just as we performed aggregate and group-and-aggregate operations above,
we can perform a co-aggregate and cogroup-and-aggregate of several
stages together. The result is a many-to-many join that matches the same
key in all the joined streams and can simultaneously perform aggregation
of the groups with the same key.

You can join any number of streams on a common key. The key can be
anything you can calculate from the data item. In the same step you can
apply an aggregate function to all the grouped items, separated by the
originating stream. The aggregate function can produce a summary value
like an average or linear trend, but it can also keep a list of all the
items, effectively avoiding actual aggregation. This way you can achieve
any kind of join: inner/outer/full, left and right. The constraint, as
already noted, is that the join condition cannot be fully general, but
must amount to matching a computed key.

Here's a simple example where we do the Word Count from two source
lists:

[source]
----
include::{javasource}/BuildComputation.java[tag=s7]
----

Note how we supplied a separate aggregate operation for each input
stage. The type of the result is `Entry<String, Tuple2<Long, Long>>`
with a correspondence between the input stages and the members of the
tuple. The result for stage-0 (the one you invoke `aggregate2` on) is
in `tuple2.f0()` and the other stage's result is in `tuple2.f1()`.

== Example: Right-Outer Join Using Co-Aggregation

Here's an example that joins streams of page visits and add-to-cart
events on the common key of user ID. It's a right-outer join, which
means all add-to-cart events will be in the output, but page-visit
events without a matching add-to-cart event won't.

[source]
----
include::{javasource}/BuildComputation.java[tag=s7a]
----

[[cogroup-builder]]
==  Co-aggregate Four or More Streams Using the Builder

If you need to join more than three streams, you'll have to use the
{jet-javadoc}/pipeline/StageWithGroupingAndWindow.html#aggregateBuilder--[builder]
object. For example, your goal may be correlating events coming from
different systems, where all the systems serve the same user base. In an
online store you may have separate event streams for product page visits,
adding-to-cart events, payments, and deliveries. You want to correlate
all the events associated with the same user. The example below
calculates statistics per category for each user:

[source]
----
include::{javasource}/BuildComputation.java[tag=s9]
----

<1> Create four source streams
<2> Obtain a builder object for the co-group transform, specify the
aggregate operation for `PageVisits`
<3> Add the co-grouped streams to the builder, specifying the aggregate
operation to perform on each
<4> Build the co-group transform, retrieve the individual aggregation
results using the tags you got in step 3

= Windowed Aggregation

Let's upgrade our batch Word Count to windowed aggregation over an
endless stream of tweets:

[source]
----
include::{javasource}/BuildComputation.java[tag=s13]
----

Now we've got a Jet job that does live tracking of words currently
trending in tweets. The sliding window definition tells Jet to aggregate
over the last minute and update its result every second.

We employed the simplest way of dealing with the notion of time: we
ignored the time when the event actually happened and just slapped the
current time on it. Usually the event contains its own timestamp and
Jet must honor it. To achieve that, you pass a function that selects the
timestamp to `addTimestamps()`, but now you must also deal with events
coming out of order. We discussed these concerns <<time-ordering, in the
previous chapter>>.

Here's an example with `Tweet` objects that carry their own timestamp:

[source]
----
include::{javasource}/BuildComputation.java[tag=s14]
----

In the line `.addTimestamps(TweetWord::timestamp,
TimeUnit.SECONDS.toMillis(5))` we specified two things: how to extract
the timestamp and how much event lag we want to tolerate. We said that
an event's timestamp can be at most five seconds behind the newest event
we already received. This also means that Jet will have to delay
emitting a windowed result until it receives an event that's five
seconds past the window's end, so the latency will typically be five
seconds or more.

Notice how we had to struggle a bit to hold on to the timestamp through
the flatmapping transformation. We needed two classes: `Tweet` and
`TweetWord`. We can avoid this need if we assign timestamps earlier on:

[source]
----
include::{javasource}/BuildComputation.java[tag=s15]
----

This will work correctly because Jet keeps the event timestamps
internally and correlates the input item to all the output items in a
`flatMap` transformation, propagating the timestamp value.

You should generally prefer setting timestamps as the first thing in
a pipeline because then the source connector will handle the watermarks
itself. The connector knows more about the structure of its source and
can produce better watermarks, with less latency and less dropped late
events.

= Kinds of Windows

Jet supports these kinds of windows:

- _tumbling_ window: a window of constant size that "tumbles" along the
time axis: the consecutive positions of the window don't overlap. If you
use a window size of 1 second, Jet will group together all events that
occur within the same second and you'll get window results for intervals
[0-1) seconds, then [1-2) seconds, and so on.

- _sliding_ window: a window of a constant size that slides along the
time axis. It slides in discrete steps of configurable size, but with
the restriction that the window size must be divisible by the sliding
step size. A typical setting is to slide by 1% of the window size. Jet
outputs the aggregation result each time the window moves on. If you use
a window of size 1 second sliding by 10 milliseconds, Jet will output
window results for intervals [0.00-1.00) seconds, then [0.01-1.01)
seconds, and so on.

- _session_ window: it captures a burst of events separated by periods
of quiescence. You define the "session timeout", i.e., the length of the
quiet period that causes the window to close. If you define a grouping
key, there is a separate, independent session window for each key.

= Rolling Aggregation

Jet supports a way to aggregate an unbounded stream without windowing:
for each input item you get the current aggregation value as output, as
if this item was the last one to be aggregated. You use the same
`AggregateOperation` implementations that work with Jet's `aggregate`
API. Note that Jet doesn't enforce processing in the order of event time;
what you get accounts for the items that Jet happens to have processed
so far.

This kind of aggregation is useful in jobs that monitor a stream for
extremes or totals. For example, the worst latency ever experienced,
the highest throughput seen, total number of transactions processed,
and so on.

In this example we get the "largest" trade seen so far in the stream
(with the highest worth in dollars). Note that rolling aggregation
outputs an item every time, not just after a new record-breaking trade.

[source]
----
include::{javasource}/BuildComputation.java[tag=s19]
----

