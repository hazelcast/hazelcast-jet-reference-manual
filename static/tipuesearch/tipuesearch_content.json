{"pages":[{"title":"Hazelcast Jet Reference Manual","text":"Welcome to the Hazelcast Jet Reference Manual. This manual includes concepts, instructions, and samples to guide you on how to use Hazelcast Jet to build applications. As the reader of this manual, you must be familiar with the Java programming language and you should have installed your preferred Integrated Development Environment (IDE).","tags":"","url":"index.html"},{"title":"Preface","text":"Naming Hazelcast Jet or Jet both refer to the same distributed data processing engine provided by Hazelcast, Inc. Hazelcast or Hazelcast IMDG both refer to Hazelcast in-memory data grid middleware. Hazelcast is also the name of the company (Hazelcast, Inc.) providing Hazelcast IMDG and Hazelcast Jet. Hazelcast Jet Architecture You can see the features for Hazelcast Jet in the following architecture diagram. Licensing Hazelcast Jet and Hazelcast Jet Reference Manual are free and provided under the Apache License, Version 2.0. Trademarks Hazelcast is a registered trademark of Hazelcast, Inc. All other trademarks in this manual are held by their respective owners. Getting Help Support for Hazelcast Jet is provided via GitHub, Hazelcast Jet mailing list and Stack Overflow. For information on the commercial support for Hazelcast Jet, please see hazelcast.com. Typographical Conventions Below table shows the conventions used in this manual. Convention Description bold font - Indicates part of a sentence that requires the reader's specific attention. - Also indicates property\/parameter values. italic font - When italicized words are enclosed with &quot;&lt;&quot; and &quot;&gt;&quot;, it indicates a variable in the command or code syntax that you must replace (for example, hazelcast-&lt;version&gt;.jar). - Note and Related Information texts are in italics. monospace Indicates files, folders, class and library names, code snippets, and inline code words in a sentence. RELATED INFORMATION Indicates a resource that is relevant to the topic, usually with a link or cross-reference. NOTE Indicates information that is of special interest or importance, for example an additional action required only in certain circumstances. element &amp; attribute Mostly used in the context of declarative configuration that you perform using an XML file. Element refers to an XML tag used to configure a feature. Attribute is a parameter owned by an element, contributing into the declaration of that element's configuration. Please see the following example.&lt;property name=&quot;property1&quot;&gt;value1&lt;\/property&gt; In this example, name is an attribute of the property element.","tags":"","url":"Preface.html"},{"title":"Introduction","text":"Hazelcast Jet is a distributed data processing engine, built for high-performance batch and stream processing. It reuses some features and services of Hazelcast In-Memory Data Grid (IMDG), but is otherwise a separate product with features not available in the IMDG. Jet also enriches the IMDG data structures such as IMap and IList with a distributed implementation of java.util.stream. With Hazelcast\u00e2\u0080\u0099s IMDG providing storage functionality, Hazelcast Jet performs parallel execution to enable data-intensive applications to operate in near real-time. Using directed acyclic graphs (DAG) to model relationships between individual steps in the data processing pipeline, Hazelcast Jet can execute both batch and stream-based data processing applications. Jet handles the parallel execution using the green thread approach to optimize the utilization of the computing resources. Breakthrough application speed is achieved by keeping both the computation and data storage in memory. The embedded Hazelcast IMDG provides elastic in-memory storage and is a great tool for storing the results of a computation or as a cache for datasets to be used during the computation. Extremely low end-to-end latencies can be achieved this way. It is extremely simple to use - in particular Jet can be fully embedded for OEMs and for Microservices \u00e2\u0080\u0093 making it is easier for manufacturers to build and maintain next generation systems. Also, Jet uses Hazelcast discovery for finding the members in the cluster, which can be used in both on-premise and cloud environments.","tags":"","url":"Introduction\/index.html"},{"title":"Data Processing Model","text":"Hazelcast Jet provides high performance in-memory data processing by modeling the computation as a Directed Acyclic Graph (DAG) where vertices represent computation and edges represent data connections. A vertex receives data from its inbound edges, performs a step in the computation, and emits data to its outbound edges. A single vertex's computation work is performed in parallel by many instances of the Processor type around the cluster. One of the major reasons to divide the full computation task into several vertices is data partitioning: the ability to split the data stream traveling over an edge into slices which can be processed independently of each other. To make this work, a function must be defined which computes the partitioning key for each item and makes all related items map to the same key. The computation engine can then route all such items to the same processor instance. This makes it easy to parallelize the computation: each processor will have the full picture for its slice of the entire stream. Edges determine how the data is routed from individual source processors to individual destination processors. Different edge properties offer precise control over the flow of data.","tags":"","url":"Introduction\/Data_Processing_Model.html"},{"title":"Clustering and Discovery","text":"Hazelcast Jet typically runs on several machines that form a cluster but it may also run on a single JVM for testing purposes. There are several ways to configure the members for discovery, explained in detail in the Hazelcast IMDG Reference Manual.","tags":"","url":"Introduction\/Clustering_and_Discovery.html"},{"title":"Members and Clients","text":"A Hazelcast Jet instance is a unit where the processing takes place. There can be multiple instances per JVM, however this only makes sense for testing. An instance becomes a member of a cluster: it can join and leave clusters multiple times during its lifetime. Any instance can be used to access a cluster, giving an appearance that the entire cluster is available locally. On the other hand, a client instance is just an accessor to a cluster and no processing takes place in it.","tags":"","url":"Introduction\/Members_and_Clients.html"},{"title":"Relationship with Hazelcast IMDG","text":"Hazelcast Jet leans on Hazelcast IMDG for cluster formation and maintenance, data partitioning, and networking. For more information on Hazelcast IMDG, see the latest Hazelcast Reference Manual. As Jet is built on top of the Hazelcast platform, there is a tight integration between Jet and IMDG. A Jet job is implemented as a Hazelcast IMDG proxy, similar to the other services and data structures in Hazelcast. The Hazelcast Operations are used for different actions that can be performed on a job. Jet can also be used with the Hazelcast Client, which uses the Hazelcast Open Binary Protocol to communicate different actions to the server instance. Reading from and Writing to Hazelcast Distributed Data Structures Jet embedds Hazelcast IMDG. Therefore, Jet can use Hazelcast IMDG maps, caches and lists on the embedded cluster as sources and sinks of data and make use of data locality. A Hazelcast IMap or ICache is distributed by partitions across the cluster and Jet members are able to efficiently read from the Map or Cache by having each member read just its local partitions. Since the whole IList is stored on a single partition, all the data will be read on the single member that owns that partition. When using a map, cache or list as a Sink, it is not possible to directly make use of data locality because the emitted key-value pair may belong to a non-local partition. In this case the pair must be transmitted over the network to the member which owns that particular partition. Jet can also use any remote Hazelcast IMDG instance via Hazelcast IMDG connector.","tags":"","url":"Introduction\/Relationship_with_Hazelcast_IMDG.html"},{"title":"Fault Detection","text":"In its current version, Hazelcast Jet can only detect a failure in one of the cluster members that was running the computation, and abort the job. A feature planned for the future is fault tolerance: the ability to go back to a saved snapshot of the computation state and resume the computation without the failed member.","tags":"","url":"Introduction\/Fault_Detection.html"},{"title":"Elasticity","text":"Hazelcast Jet supports the scenario where a new member joins the cluster while a job is running. Currently the ongoing job will not be re-planned to start using the member, though; this is on the roadmap for a future version. The new member can also leave the cluster while the job is running and this won't affect its progress. One caveat is the special kind of member allowed by the Hazelcast IMDG: a lite member. These members don't get any partitions assigned to them and will malfunction when attempting to run a DAG with partitioned edges. Lite members should not be allowed to join a Jet cluster.","tags":"","url":"Introduction\/Elasticity.html"},{"title":"Getting Started","text":"This chapter explains how to start using Hazelcast Jet. It also describes the executable files in the downloaded distribution package. Requirements Hazelcast Jet requires a minimum JDK version of 8. Using Maven and Gradle The easiest way to start using Hazelcast Jet is to add it as a dependency to your project. You can find Hazelcast Jet in Maven repositories. Add the following lines to your pom.xml: &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.hazelcast.jet&lt;\/groupId&gt; &lt;artifactId&gt;hazelcast-jet&lt;\/artifactId&gt; &lt;version&gt;0.3&lt;\/version&gt; &lt;\/dependency&gt; &lt;\/dependencies&gt; If you prefer to use Gradle, execute the following command: compile 'com.hazelcast.jet:hazelcast-jet:0.3' Downloading Alternatively, you can download the latest distribution package for Hazelcast Jet and add the hazelcast-jet-&lt;version&gt;.jar file to your classpath. Distribution Package The distribution package contains the following scripts to help you get started with Hazelcast Jet: bin\/start-jet.sh and bin\/start-jet.bat start a new Jet member in the current directory. bin\/stop-jet.sh and bin\/stop-jet.bat stop the member started in the current directory. bin\/submit-jet.sh and bin\/submit-jet.bat submit a Jet computation job that was packaged in a self-contained JAR file. bin\/cluster.sh provides basic functionality for Hazelcast cluster manager, such as changing the cluster state, shutting down the cluster or forcing the cluster to clean its persisted data. NOTE: start-jet.sh \/ start-jet.bat scripts lets you start one Jet member per folder. To start a new instance, please unzip the package in a new folder.","tags":"","url":"Getting_Started.html"},{"title":"Hazelcast Jet 101 -Word Counting Batch Job","text":"For the first introductory example we'll go through the steps of creating a distributed word counting job in Hazelcast Jet. We have a dataset consisting of lines of text; we want to find the number of occurrences of each word in it. In essence, our computation is this: List&lt;String&gt; lines = ... \/\/ a pre-existing list Map&lt;String, Long&gt; counts = new HashMap&lt;&gt;(); for (String line : lines) { for (String word : line.split(&quot;\\\\W+&quot;)) { counts.compute(word.toLowerCase(), (w, c) -&gt; c == null ? 1L : c + 1); } } We'll show you how to model this computation as a directed graph of computation steps; we'll point out the concerns that arise when trying to make it work on a huge amount of data distributed across several machines, and we'll show you how to meet those concerns with Jet. Note that here we are describing a batch job: the input is finite and present in full before the job starts. Later on we'll present a streaming job that keeps processing an infinite stream forever, transforming it into another infinite stream.","tags":"","url":"Hazelcast_Jet_101_-Word_Counting_Batch_Job\/index.html"},{"title":"Modeling Word Count in terms of a DAG","text":"The word count computation can be roughly divided into three steps: Read a line from the map (&quot;source&quot; step) Split the line into words (&quot;tokenizer&quot; step) Update the running totals for each word (&quot;accumulator&quot; step) We can represent these steps as a DAG: In the simplest case the computation inside each vertex can be executed in turn in a single-threaded environment; however just by modeling the computation as a DAG we've split the work into isolated steps with clear data interfaces between them. This means each vertex can have its own thread and they can communicate over concurrent queues: This achieves a pipelined architecture: while the tokenizer is busy with the regex work, the accumulator is updating the map using the data the tokenizer is done with; and the source and sink stages are pumping the data from\/to the environment. Our design is now able to engage more than one CPU core and will complete that much sooner; however we're still limited by the number of vertices. We'll be able utilize two or three cores regardless of how many are available. To move forward we must try to parallelize the work of each individual vertex. Given that our input is an in-memory list of lines, the bottleneck occurs in the processing stages (tokenizing and accumulating). Let's first attack the tokenizing stage: it is a so-called &quot;embarassingly parallelizable&quot; task because the processing of each line is completely self-contained. At this point we have to make a clear distinction between the notions of vertex and processor: there can be several processors doing the work of a single vertex. Let's add another tokenizing processor: The input processor can now use all the available tokenizers as a pool and submit to any one whose queue has some room. The next step is parallelizing the accumulator vertex, but this is trickier: all occurences of the same word must go to the same accumulator. The input to the accumulator must be partitioned by word so that each processor is responsible for a non-overlapping subset of the words. In Jet we'll use a partitioned edge between the tokenizer and the accumulator: As a word is emitted from the tokenizer, it goes through a &quot;switchboard&quot; stage where it's routed to the correct downstream processor. To determine where a word should be routed we can for example calculate its hashcode and use the lowest bit to address either accumulator 0 or accumulator 1. At this point we have a blueprint for a fully functional parallelized computation job which can max out all the CPU cores given enough instances of tokenizing and accumulating processors. The next challenge is making this work across machines. For starters, our input can no longer be a simple in-memory list because that would mean each machine processes the same data. To exploit a cluster as a unified computation device, each node must observe only a slice of the dataset. Given that a Jet instance is also a fully functional Hazelcast instance and a Jet cluster is also a Hazelcast cluster, the natural choice is to pre-load our data into an IMap, which will be automatically partitioned and distributed between the nodes. Now each Jet node can just read the slice of data that was stored locally on it. When run in a cluster, Jet will instantiate a replica of the whole DAG on each node, so in total there will be two source processors, four tokenizers, and so on. The trickiest part is the partitioned edge between tokenizer and accumulator: each accumulator is supposed to receive its own subset of words. That means that, for example, a word emitted from tokenizer 0 will have to travel across the network to reach accumulator 3, if that's the one that happens to own it. On average we can expect every other word to need network transport, causing both serious network traffic and serialization\/deserialization CPU load. There is a simple trick we can employ to avoid most of this traffic: we'll make our accumulators process only local data, coming up with local word counts, and we'll introduce another vertex downstream of accumulator, called the combiner, which will just total up the local counts. This way we reduce traffic from O(totalWords) to O(distinctWords). Given that there are only so many words in a language, this is in fact a reduction from O(n) in input size to O(1). The more text we process, the larger our savings in network traffic. Jet distinguishes between local and distributed edges, so we'll use a local partitioned edge for tokenizer-&gt;accumulator and a distributed partitioned edge for accumulator-&gt;combiner. With this move we've finalized our DAG design, which can be illustrated by the following diagram:","tags":"","url":"Hazelcast_Jet_101_-Word_Counting_Batch_Job\/Modeling_Word_Count_in_terms_of_a_DAG.html"},{"title":"Implementing And Running the DAG","text":"Now that we've come up with a good DAG design, it's time to implement it using the Jet DAG API. We'll present this in several steps: Start a Jet cluster; populate an IMap with sample data; build the Jet DAG; submit it for execution. To start a new Jet cluster, we need to start some Jet instances. Typically these would be started on separate machines, but for the purposes of this tutorial we will be using the same JVM for both of the instances. We can start the instances as shown below: public class WordCount { public static void main(String[] args) { JetInstance jet = Jet.newJetInstance(); Jet.newJetInstance(); } } These two members should automatically discover each other using IP multicast and form a cluster. You should see log output similar to the following: Members [2] { Member [10.0.1.3]:5701 - f1e30062-e87e-4e97-83bc-6b4756ef6ea3 Member [10.0.1.3]:5702 - d7b66a8c-5bc1-4476-a528-795a8a2d9d97 this } This means the members successfully formed a cluster. Don't forget to shut down the members afterwards, by adding the following as the last line of your application: Jet.shutdownAll(); This must be executed unconditionally, even in the case of an exception; otherwise your Java process will stay alive because Jet has started its internal threads: public class WordCount { public static void main(String[] args) { try { JetInstance jet = Jet.newJetInstance(); Jet.newJetInstance(); ... your code here... } finally { Jet.shutdownAll(); } } } As explained earlier, we'll use an IMap as our data source. Let's give it some sample data: IMap&lt;Integer, String&gt; map = jet.getMap(&quot;lines&quot;); map.put(0, &quot;It was the best of times,&quot;); map.put(1, &quot;it was the worst of times,&quot;); map.put(2, &quot;it was the age of wisdom,&quot;); map.put(3, &quot;it was the age of foolishness,&quot;); map.put(4, &quot;it was the epoch of belief,&quot;); map.put(5, &quot;it was the epoch of incredulity,&quot;); map.put(6, &quot;it was the season of Light,&quot;); map.put(7, &quot;it was the season of Darkness&quot;); map.put(8, &quot;it was the spring of hope,&quot;); map.put(9, &quot;it was the winter of despair,&quot;); map.put(10, &quot;we had everything before us,&quot;); map.put(11, &quot;we had nothing before us,&quot;); map.put(12, &quot;we were all going direct to Heaven,&quot;); map.put(13, &quot;we were all going direct the other way --&quot;); map.put(14, &quot;in short, the period was so far like the present period, that some of &quot; + &quot;its noisiest authorities insisted on its being received, for good or for &quot; + &quot;evil, in the superlative degree of comparison only.&quot;); Let's move on to the code needed to build and run the DAG. The first step is to create a DAG and source vertex: DAG dag = new DAG(); Vertex source = dag.newVertex(&quot;source&quot;, Processors.readMap(&quot;lines&quot;)); This is a simple vertex which will read the lines from the IMap and emit items of type Map.Entry&lt;Integer, String&gt; to the next vertex. The key of the entry is the line number, and the value is the line itself. The built-in map-reading processor will do just what we want: on each member it will read only the data local to that member. The next vertex is the tokenizer, which does a simple &quot;flat-mapping&quot; operation (transforms one input item into zero or more output items). The low-level support for such a processor is a part of Jet's library, we just need to provide the mapping function: \/\/ (lineNum, line) -&gt; words Pattern delimiter = Pattern.compile(&quot;\\\\W+&quot;); Vertex tokenizer = dag.newVertex(&quot;tokenizer&quot;, Processors.flatMap((Entry&lt;Integer, String&gt; e) -&gt; Traversers.traverseArray(delimiter.split(e.getValue().toLowerCase())) .filter(word -&gt; !word.isEmpty())) ); This creates a processor that applies the given function to each incoming item. The function must return a Traverser over all the output items, which is an abstraction that traverses a sequence of non-null items. Its purpose is equivalent to the standard Java Iterator, but avoids the cumbersome two-method API. Traverser is a simple functional interface with just one method to implement: next(). Specifically, our processor accepts items of type Entry&lt;Integer, String&gt;, splits the entry value into lowercase words, and emits all non-empty words. The next vertex will do the grouping of the words and emit the count for each word. We can use the built-in groupAndAccumulate processor. \/\/ word -&gt; (word, count) Vertex accumulator = dag.newVertex(&quot;accumulator&quot;, Processors.groupAndAccumulate(() -&gt; 0L, (count, x) -&gt; count + 1) ); This processor maintains a hashtable that maps each distinct item to the accumulated value. It takes two functions: the first one produces the initial value and is called when observing a new distinct item; the second one gets the previous value for the item and returns a new value. The above function definitions can be seen to have the effect of counting the items. The processor emits nothing until it has received all the input, and at that point it emits the hashtable as a stream of Entry&lt;String, Long&gt;. Next is the combining step which computes the grand totals from individual members' contributions. This is the code: \/\/ (word, count) -&gt; (word, count) Vertex combiner = dag.newVertex(&quot;combiner&quot;, Processors.groupAndAccumulate( Entry&lt;String, Long&gt;::getKey, () -&gt; 0L, (count, wordAndCount) -&gt; count + wordAndCount.getValue()) ); It's very similar to the previous accumulator vertex, but instead of the simple increment here we have to add the entry's value to the total. The final vertex is the output \u00e2\u0080\u0094 we want to store the output in another IMap: Vertex sink = dag.newVertex(&quot;sink&quot;, Processors.writeMap(&quot;counts&quot;)); Now that we have all the vertices, we must connect them into a graph and specify the edge type as discussed in the previous section. Here's all the code at once: dag.edge(between(source, tokenizer)) .edge(between(tokenizer, accumulator) .partitioned(DistributedFunctions.wholeItem(), Partitioner.HASH_CODE)) .edge(between(accumulator, combiner) .distributed() .partitioned(DistributedFunctions.entryKey())) .edge(between(combiner, sink)); Let's take a closer look at some of the connections between the vertices. First, source and tokenizer: .edge(between(tokenizer, accumulator) .partitioned(DistributedFunctions.wholeItem(), Partitioner.HASH_CODE)) The edge between the tokenizer and accumulator is local partitioned. For each word, on each member there will be a processor responsible for it so that no items must travel across the network. In the partitioned() call we specify two things: the function that extracts the partitioning key (in this case the built-in wholeItem() function, which is simply the identity function with a descriptive name), and the policy object that decides how to compute the partition ID from the key. Here we use the built-in HASH_CODE, which will derive the ID from Object.hashCode(). This is always safe to use on a local edge. .edge(between(accumulator, combiner) .distributed() .partitioned(DistributedFunctions.entryKey())) The edge between accumulator and combiner is distributed partitioned: for each word there is a single combiner processor in the whole cluster responsible for it and items will be sent over the network if needed. The partitioning key is again the word, but here it is the key part of the Map.Entry&lt;String, Long&gt;. For demonstration purposese we are using the default partitioning policy here (default Hazelcast partitioning). It is the slower-but-safe choice on a distributed edge. Detailed inspection shows that hashcode-based partitioning would be safe as well because all of String, Long, and Map.Entry have the hash function specified in their Javadoc. To run the DAG and print out the results, we simply do the following: jet.newJob(dag).execute().get(); System.out.println(jet.getMap(&quot;counts&quot;).entrySet()); The final output should look like the following: [heaven=1, times=2, of=12, its=2, far=1, light=1, noisiest=1, the=14, other=1, incredulity=1, worst=1, hope=1, on=1, good=1, going=2, like=1, we=4, was=11, best=1, nothing=1, degree=1, epoch=2, all=2, that=1, us=2, winter=1, it=10, present=1, to=1, short=1, period=2, had=2, wisdom=1, received=1, superlative=1, age=2, darkness=1, direct=2, only=1, in=2, before=2, were=2, so=1, season=2, evil=1, being=1, insisted=1, despair=1, belief=1, comparison=1, some=1, foolishness=1, or=1, everything=1, spring=1, authorities=1, way=1, for=2] The full version of this sample, with some more can be found at the Hazelcast Jet code samples repository.","tags":"","url":"Hazelcast_Jet_101_-Word_Counting_Batch_Job\/Implementing_And_Running_the_DAG.html"},{"title":"Understanding Jet Architecture and API","text":"This chapter provides an overview of Jet's architecture and API concepts.","tags":"","url":"Understanding_Jet_Architecture_and_API\/index.html"},{"title":"DAG","text":"At the core of Jet is the distributed computation engine based on the paradigm of a directed acyclic graph (DAG). In this graph, vertices are units of data processing and edges are units of data routing and transfer. Each vertex's computation is implemented by a subtype of the Processor interface. On each member there are one or more instances of the processor running in parallel for a single vertex; their number is configured using its localParallelism attribute. Generally the processor is implemented by the user, but there are some ready-made implementations in Jet's library for common operations like flatMap and groupBy. Data sources and sinks are implemented as Processors as well and are used for the terminal vertices of the DAG. A source can be distributed, which means that on each member of the Jet cluster a different slice of the full data set will be read. Similarly, a sink can also be distributed so each member can write a slice of the result data to its local storage. Data partitioning is used to route each slice to its target member. Examples of distributed sources supported by Jet are HDFS files and Hazelcast's IMap, ICache and IList. Edges transfer data from one vertex to the next and contain the partitioning logic which ensures that each item is sent to its target processor. After a Job is created, the DAG is replicated to the whole Jet cluster and executed in parallel on each member. Execution is done on a user-configurable number of threads which use work stealing to balance the amount of work being done on each thread. Each worker thread has a list of tasklets it is in charge of and as tasklets complete at different rates, the remaining ones are moved between workers to keep the load balanced. Each instance of a Processor is wrapped in one tasklet which is repeatedly executed until it reports it is done. A vertex with a parallelism of 8 running on 4 nodes would have a total of 32 tasklets running at the same time. Each node will have the same number of tasklets running. When a request to execute a Job is made, the corresponding DAG and additional resources are deployed to the Jet cluster. An execution plan for the DAG is built on each node, which creates the associated tasklets for each Vertex and connects them to their inputs and outputs. Jet uses Single Producer\/Single Consumer ringbuffers to transfer the data between processors on the same member. They are data-type agnostic, so any data type can be used to transfer the data between vertices. Ringbuffers, being bounded queues, introduce natural back pressure into the system; if a consumer\u00e2\u0080\u0099s ringbuffer is full, the producer will have to back off until it can enqueue the next item. When data is sent to another member over the network, there is no natural back pressure, so Jet uses explicit signaling in the form of adaptive receive windows.","tags":"","url":"Understanding_Jet_Architecture_and_API\/DAG.html"},{"title":"Job","text":"A Job is the unit of work which is executed. A Job is described by a DAG, which describes the computation to be performed, and the inputs and outputs of the computation. Job is a handle to the execution of a DAG. To create a job, supply the DAG to a previously created JetInstance as shown below: JetInstance jet = Jet.newJetInstance(); \/\/ or Jet.newJetClient(); DAG dag = new DAG(); dag.newVertex(..); jet.newJob(dag).execute().get(); As hinted in the code example, the job submission API is identical whether you use it from a client machine or directly on an instance of a Jet cluster member. This works because the Job instance is serializable and the client can send it over the network when submitting the job. The same Job instance can be submitted for execution many times. Job execution is asynchronous. The execute() call returns as soon as the Jet cluster has been contacted and the serialized job is sent to it. The user gets a Future which can be inspected or waited on to find out the outcome of a computation job. It is also cancelable and can send a cancelation command to the Jet cluster. Note that the Future only signals the status of the job, it does not contain the result of the computation. The DAG explicitly models the storing of results via its sink vertices. Typically the results will be in a Hazelcast map or another structure and have to be accessed by their own API after the job is done. Deploying the Resources If the Jet cluster has not been started with all the job's computation code already on the classpath, you have to deploy the code together with the Job instance: JobConfig config = new JobConfig(); config.addJar(&quot;..&quot;); jet.newJob(dag, config).execute().get(); When reading and writing data to the underlying Hazelcast IMDG instance, keep in mind that the deployed code is available only within the scope of the executing Jet job.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Job.html"},{"title":"Vertex","text":"Vertex is the main unit of work in a Jet computation. Conceptually, it receives input from its inbound edges and emits data to its outbound edges. Practically, it is a number of Processor instances which receive each of its own part of the full stream traveling over the inbound edges, and likewise emits its own part of the full stream going down the outbound edges. Edge Ordinal An edge is connected to a vertex with a given ordinal, which identifies it to the vertex and its processors. When a processor receives an item, it knows the ordinal of the edge on which the item came in. Things are similar on the outbound side: the processor emits an item to a given ordinal, but also has the option to emit the same item to all ordinals. This is the most typical case and allows easy replication of a data stream across several edges. In the DAG-building API the default value of the ordinal is 0. There must be no gaps in ordinal assignment, which means a vertex will have inbound edges with ordinals 0..N and outbound edges with ordinals 0..M. Source and Sink Jet uses only one kind of vertex, but in practice there is an important distinction between the following: internal vertex which accepts input and transforms it into output, source vertex which generates output without receiving anything, sink vertex which consumes input and does not emit anything. Sources and sinks must interact with the environment to store\/load data, making their implementation more involved compared to the internal vertices, whose logic is self-contained. Local and Global Parallelism The vertex is implemented by one or more instances of Processor on each member. Each vertex can specify how many of its processors will run per cluster member using the localParallelism property; every member will have the same number of processors. A new Vertex instance has this property set to -1, which requests to use the default value equal to the configured size of the cooperative thread pool. The latter defaults to Runtime.availableProcessors(). The global parallelism of the vertex is also an important value, especially in terms of the distribution of partitions among processors. It is equal to local parallelism multiplied by the cluster size.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Vertex.html"},{"title":"Processor","text":"Processor is the main type whose implementation is up to the user: it contains the code of the computation to be performed by a vertex. There are a number of Processor building blocks in the Jet API which allow you to just specify the computation logic, while the provided code handles the processor's cooperative behavior. Please refer to the AbstractProcessor section. A processor's work can be conceptually described as follows: &quot;receive data from zero or more input streams and emit data into zero or more output streams.&quot; Each stream maps to a single DAG edge (either inbound or outbound). There is no requirement on the correspondence between input and output items; a processor can emit any data it sees fit, including none at all. The same Processor abstraction is used for all kinds of vertices, including sources and sinks.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Processor\/index.html"},{"title":"Cooperative Multithreading","text":"Cooperative multithreading is one of the core features of Jet and can be roughly compared to green threads. It is purely a library-level feature and does not involve any low-level system or JVM tricks; the Processor API is simply designed in such a way that the processor can do a small amount of work each time it is invoked, then yield back to the Jet engine. The engine manages a thread pool of fixed size and on each thread, the processors take their turn in a round-robin fashion. The point of cooperative multithreading is better performance. Several factors contribute to this: the overhead of context switching between processors is much lower since the operating system's thread scheduler is not involved; the worker thread driving the processors stays on the same core for longer periods, preserving the CPU cache lines; worker thread has direct knowledge of the ability of a processor to make progress (by inspecting its input\/output buffers). Processor instances are cooperative by default. The processor can opt out of cooperative multithreading by overriding isCooperative() to return false. Jet will then start a dedicated thread for it. Requirements To maintain an overall good throughput, a cooperative processor must take care not to hog the thread for too long (a rule of thumb is up to a millisecond at a time). Jet's design strongly favors cooperative processors and most processors can and should be implemented to fit these requirements. The major exception are sources and sinks because they often have no choice but calling into blocking I\/O APIs.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Processor\/Cooperative_Multithreading.html"},{"title":"Outbox","text":"The processor sends its output items to its Outbox, which has a separate bucket for each outbound edge. The buckets have limited capacity and will refuse an item when full. A cooperative processor should be implemented such that when its item is rejected by the outbox, it saves its processing state and returns from the processing method. The execution engine will then drain the outbox buckets. By contrast, a non-cooperative processor gets an auto-flushing, blocking outbox that never rejects an item. This can be leveraged to simplify the processor's implementation; however the simplification alone should never be the reason to declare a processor non-cooperative.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Processor\/Outbox.html"},{"title":"Data-Processing Callbacks","text":"Two callback methods are involved in data processing: process() and complete(). Implementations of these methods can be stateful and do not need to be thread-safe. A single instance will be called by a single thread at a time, although not necessarily always the same thread. process() Jet passes the items received over a given edge by calling process(ordinal, inbox). All items received since the last process() call are in the inbox, but also all the items the processor hasn't removed in a previous process() call. There is a separate instance of Inbox for each inbound edge, so any given process() call involves items from only one edge. The processor must not remove an item from the inbox until it has fully processed it. This is important with respect to the cooperative behavior: the processor may not be allowed to emit all items corresponding to a given input item and may need to return from the process() call early, saving its state. In such a case the item should stay in the inbox so Jet knows the processor has more work to do even if no new items are received. completeEdge() Eventually each edge will signal that its data stream is exhausted. When this happens, Jet calls the processor's completeEdge() with the ordinal of the completed edge. The processor may want to emit any number of items upon this event, and it may be prevented from emitting all due to a full outbox. In this case it may return false and will be called again later. complete() Jet calls complete() after all the input edges are exhausted. It is the last method to be invoked on the processor before disposing of it. Typically this is where the processor emits the results of an accumulating operation. If it can't emit everything in a given call, it should return false and will be called again later.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Processor\/Data-Processing_Callbacks.html"},{"title":"Creating and Initializing Jobs","text":"These are the steps taken to create and initialize a Jet job: The user builds the DAG and submits it to the local Jet client instance. The client instance serializes the DAG and sends it to a member of the Jet cluster. This member becomes the coordinator for this Jet job. The coordinator deserializes the DAG and builds an execution plan for each member. The coordinator serializes the execution plans and distributes each to its target member. Each member acts upon its execution plan by creating all the needed tasklets, concurrent queues, network senders\/receivers, etc. The coordinator sends the signal to all members to start job execution. The most visible consequence of the above process is the ProcessorMetaSupplier type: you must provide one for each Vertex. In Step 3, the coordinator deserializes the meta-supplier as a constituent of the DAG and asks it to create ProcessorSupplier instances which go into the execution plans. A separate instance of ProcessorSupplier is created specifically for each member's plan. In Step 4, the coordinator serializes these and sends each to its member. In Step 5 each member deserializes its ProcessorSupplier and asks it to create as many Processor instances as configured by the vertex's localParallelism property. This process is so involved because each Processor instance may need to be differently configured. This is especially relevant for processors driving a source vertex: typically each one will emit only a slice of the total data stream, as appropriate to the partitions it is in charge of. ProcessorMetaSupplier This type is designed to be implemented by the user, but the Processors utility class provides implementations covering most cases. You may need custom meta-suppliers primarily to implement a custom data source or sink. Instances of this type are serialized and transferred as a part of each Vertex instance in a DAG. The coordinator member deserializes it to retrieve ProcessorSuppliers. Before being asked for ProcessorSuppliers, the meta-supplier is given access to the Hazelcast instance so it can find out the parameters of the cluster the job will run on. Most typically, the meta-supplier in the source vertex will use the cluster size to control the assignment of data partitions to each member. ProcessorSupplier Usually this type will be custom-implemented in the same cases where its meta-supplier is custom-implemented and complete the logic of a distributed data source's partition assignment. It supplies instances of Processor ready to start executing the vertex's logic. Please see the Implementing Custom Sources and Sinks section for more guidance on how these interfaces can be implemented.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Creating_and_Initializing_Jobs.html"},{"title":"AbstractProcessor","text":"AbstractProcessor is a convenience class designed to deal with most of the boilerplate in implementing the full Processor API. The first line of convenience are the tryProcessN() methods which receive one item at a time, thus eliminating the need to write a suspendable loop over the input items. There is a separate method specialized for each edge from 0 to 4 (tryProcess0..tryProcess4) and a catch-all method tryProcessAny(ordinal, item). If the processor doesn't need to distinguish between the inbound edges, the latter method is a good match; otherwise, it is simpler to implement one or more of the ordinal-specific methods. The catch-all method is also the only way to access inbound edges beyond ordinal 4, but such cases are very rare in practice. A major complication arises from the requirement to observe the outbox limits during a single processing step. If the processor emits many items per step, the loop doing this must support being suspended at any point and resumed later. This need arises in two typical cases: when a single input item maps to many output items, when the processor performs an accumulating operation and emits its results in the final step. AbstractProcessor provides the method emitFromTraverser to support the latter and there is additional support for the former with the nested class FlatMapper. These work with the Traverser abstraction to cooperatively emit a user-provided sequence of items.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Convenience_API_to_Implement_a_Processor\/AbstractProcessor.html"},{"title":"Traverser","text":"Traverser is a very simple functional interface whose shape matches that of a Supplier, but with a contract specialized for the traversal over a sequence of non-null items: each call to its next() method returns another item of the sequence until exhausted, then keeps returning null. The point of this type is the ability to implement traversal over any kind of dataset or lazy sequence with minimum hassle: often just by providing a one-liner lambda expression. This makes it very easy to integrate with Jet's convenience APIs for cooperative processors. Traverser also supports some default methods that facilitate building a simple transformation layer over the underlying sequence: map, filter, flatMap, etc.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Convenience_API_to_Implement_a_Processor\/Traverser.html"},{"title":"Simple Example","text":"The following example shows how you can implement a simple flatmapping processor: public class ItemAndSuccessorP extends AbstractProcessor { private final FlatMapper&lt;Integer, Integer&gt; flatMapper = flatMapper(i -&gt; traverseIterable(asList(i, i + 1))); @Override protected boolean tryProcess(int ordinal, Object item) { return flatMapper.tryProcess((int) item); } } For each received Integer item this processor emits the item and its successor. It does not differentiate between inbound edges (treats data from all edges the same way) and emits each item to all outbound edges connected to its vertex.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Convenience_API_to_Implement_a_Processor\/Simple_Example.html"},{"title":"Processor Utility Class","text":"As a further layer of convenience, there are some ready-made Processor implementations. These are the broad categories: Sources and sinks for Hazelcast IMap, ICache and IList. Processors with flatMap-type logic, including map, filter, and the most general flatMap. Processors that perform a reduction operation after grouping items by key. These come in two flavors: a. Accumulate: reduce by transforming an immutable value. b. Collect: reduce by updating a mutable result container. Please refer to the Processors Javadoc for further details.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Convenience_API_to_Implement_a_Processor\/Processor_Utility_Class.html"},{"title":"Edge","text":"An edge represents a link between two vertices in the DAG. Conceptually, data flows between two vertices along an edge; practically, each processor of the upstream vertex contributes to the overall data stream over the edge and each processor of the downstream vertex receives a part of that stream. For any given pair of vertices, there can be at most one edge between them. Several properties of the Edge control the routing from upstream to downstream processors.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/index.html"},{"title":"Priority","text":"By default the processor receives items from all inbound edges as they arrive. However, there are important cases where the reception of one edge must be delayed until all other edges are consumed in full. A major example is a join operation. Collating items from several edges by a common key implies buffering the data from all edges except one before emitting any results. Often there is one edge with much more data than the others and this one does not need to be buffered if all the other data is ready. Edge consumption order is controlled by the priority property. Edges are sorted by their priority number (ascending) and consumed in that order. Edges with the same priority are consumed without particular ordering (as the data arrives).","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/Priority.html"},{"title":"Local and Distributed Edges","text":"A major choice to make in terms of data routing is whether the candidate set of target processors is unconstrained, encompassing all processors across the cluster, or constrained to just those running on the same cluster member. This is controlled by the distributed property of the edge. By default the edge is local and calling the distributed() method removes this restriction. With appropriate DAG design, network traffic can be minimized by employing local edges. Local edges are implemented with the most efficient kind of concurrent queue: single-producer, single-consumer bounded queue. It employs wait-free algorithms on both sides and avoids volatile writes by using lazySet.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/Local_and_Distributed_Edges.html"},{"title":"Forwarding Patterns","text":"The forwarding pattern decides which of the processors in the candidate set to route each particular item to. Variable Unicast This is the default forwarding pattern. For each item a single destination processor is chosen with no further restrictions on the choice. The only guarantee given by this pattern is that the item will be received by exactly one processor, but typically care will be taken to &quot;spray&quot; the items equally over all the reception candidates. This choice makes sense when the data does not have to be partitioned, usually implying a downstream vertex which can compute the result based on each item in isolation. Broadcast A broadcasting edge sends each item to all candidate receivers. This is useful when some small amount of data must be broadcast to all downstream vertices. Usually such vertices will have other inbound edges in addition to the broadcasting one, and will use the broadcast data as context while processing the other edges. In such cases the broadcasting edge will have a raised priority. There are other useful combinations, like a parallelism-one vertex that produces the same result on each member. Partitioned A partitioned edge sends each item to the one processor responsible for the item's partition ID. On a distributed edge, this processor will be unique across the whole cluster. On a local edge, each member will have its own processor for each partition ID. Each processor can be assigned to multiple partitions. The global number of partitions is controlled by the number of partitions in the underlying Hazelcast IMDG configuration. Please refer to Hazelcast Reference Manual for more information about Hazelcast IMDG partitioning. This is the default algorithm to determine the partition ID of an item: Apply the keyExtractor function defined on the edge to retrieve the partitioning key. Serialize the partitioning key to a byte array using Hazelcast serialization. Apply Hazelcast's standard MurmurHash3-based algorithm to get the key's hash value. Partition ID is the hash value modulo the number of partitions. The above procedure is quite CPU-intensive, but has the essential property of giving repeatable results across all cluster members, which may be running on disparate JVM implementations. Another common choice is to use Java's standard Object.hashCode(). It is often significantly faster. However, it is not a safe strategy in general because hashCode()'s contract does not require repeatable results across JVMs, or even different instances of the same JVM version. You can provide your own implementation of Partitioner to gain full control over the partitioning strategy. All to One The all-to-one forwarding pattern is a special-case of the partitioned pattern where all items are assigned to the same partition ID, randomly chosen at the job initialization time. This will direct all items to the same processor.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/Forwarding_Patterns.html"},{"title":"Buffered Edges","text":"In some special cases, unbounded data buffering must be allowed on an edge. Consider the following scenario: A vertex sends output to two edges, creating a fork in the DAG. The branches later rejoin at a downstream vertex which assigns different priorities to its two inbound edges. Since the data for both edges is generated simultaneously, and since the lower-priority edge will apply backpressure while waiting for the higher-priority edge to be consumed in full, the upstream vertex will not be allowed to emit its data and a deadlock will occur. The deadlock is resolved by activating the unbounded buffering on the lower-priority edge.","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/Buffered_Edges.html"},{"title":"Tuning Edges","text":"Edges have some configuration properties which can be used for tuning how the items are transmitted. The following options are available: Name Description Default Value High Water Mark A Processor deposits its output items to its Outbox. It is an unbounded buffer, but has a \"high water mark\" which should be respected by a well-behaving processor. When its outbox reaches the high water mark,the processor should yield control back to its caller. 2048 Queue Size When data needs to travel between two processors on the same cluster member, it is sent over a concurrent single-producer, single-consumer (SPSC) queue of fixed size. This options controls the size of the queue. Since there are several processors executing the logic of each vertex, and since the queues are SPSC, there will be a total of senderParallelism * receiverParallelism queues representing the edge on each member. Care should be taken to strike a balance between performance and memory usage. 1024 Packet Size Limit For a distributed edge, data is sent to a remote member via Hazelcast network packets. Each packet is dedicated to the data of a single edge, but may contain any number of data items. This setting limits the size of the packet in bytes. Packets should be large enough to drown out any fixed overheads, but small enough to allow good interleaving with other packets. Note that a single item cannot straddle packets, therefore the maximum packet size can exceed the value configured here by the size of a single data item. This setting has no effect on a non-distributed edge. 16384 Receive Window Multiplier For each distributed edge the receiving member regularly sends flow-control (\"ack\") packets to its sender which prevent it from sending too much data and overflowing the buffers. The sender is allowed to send the data one receive window further than the last acknowledged byte and the receive window is sized in proportion to the rate of processing at the receiver. Ack packets are sent in regular intervals and the receive window multiplier sets the factor of the linear relationship between the amount of data processed within one such interval and the size of the receive window. To put it another way, let us define an ackworth to be the amount of data processed between two consecutive ack packets. The receive window multiplier determines the number of ackworths the sender can be ahead of the last acked byte. This setting has no effect on a non-distributed edge. 3","tags":"","url":"Understanding_Jet_Architecture_and_API\/Edge\/Tuning_Edges.html"},{"title":"Tutorial - Building an Inverted TF-IDF Index with Core API","text":"In this tutorial we'll cover most of the topics involved in building a batch processing job with Jet's Core API. We'll show you how to decide on processor parallelism, partitioning and forwarding patterns on edges, and how to optimally leverage the Core API to build the vertex logic with minimum boilerplate. The full code is available at the hazelcast-jet-code-samples repository: TfIdfJdkStreams.java TfIdf.java Our example, the inverted index, is a basic data structure in the domain of full-text search. The goal is to be able to quickly find the documents that contain a given set of search terms, and to sort them by relevance. To understand it we'll need to throw in some terminology... A document is treated as a list of words that has a unique ID. It is useful to define the notion of a document index which maps each document ID to the list of words it contains. We won't build this index; it's just for the sake of explanation. The inverted index is the inverse of the document index: it maps each word to the list of documents that contain it. This is the fundamental building block in our search algorithm: it will allow us to find in O(1) time all documents relevant to a search term. In the inverted index, each entry in the list is assigned a TF-IDF score which quantifies how relevant the document is to the search request. Let DF (document frequency) be the length of the list: the number of documents that contain the word. Let D be the total number of documents that were indexed. IDF (inverse document frequency) is equal to log(D\/DF). TF (term frequency) is the number of occurrences of the word in the document. TF-IDF score is simply the product of TF * IDF. Note that IDF is a property of the word itself: it quantifies the relevance of each entered word to the search request as a whole. The list of entered words can be perceived as a list of filtering functions that we apply to the full set of documents. A more relevant word will apply a stronger filter. Specifically, common words like &quot;the&quot;, &quot;it&quot;, &quot;on&quot; act as pure &quot;pass-through&quot; filters and consequently have an IDF of zero, making them completely irrelevant to the search. TF, on the other hand, is the property of the combination of word and document, and tells us how relevant the document is to the word, regardless of the relevance of the word itself. When the user enters a search phrase: each individual term from the phrase is looked up in the inverted index; an intersection is found of all the lists, resulting in the list of documents that contain all the words; each document is scored by summing the TF-IDF contributions of each word; the result list is sorted by score (descending) and presented to the user. Let's have a look at a specific search phrase: the man in the black suit murdered the king The list of documents that contain all the above words is quite long... how do we decide which are the most relevant? The TF-IDF logic will make those stand out that have an above-average occurrence of words that are generally rare across all documents. For example, &quot;murdered&quot; occurs in far fewer documents than &quot;black&quot;... so given two documents where one has the same number of &quot;murdered&quot; as the other one has of &quot;black&quot;, the one with &quot;murdered&quot; wins because its word is more salient in general. On the other hand, &quot;suit&quot; and &quot;king&quot; might have a similar IDF, so the document that simply contains more of both wins. Also note the limitation of this technique: a phrase is treated as just the sum of its parts; a document may contain the exact phrase and this will not affect its score.","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/index.html"},{"title":"Building Inverted Index with Java Streams","text":"To warm us up, let's see what it takes to build the inverted index with just thread parallelism and without the ability to scale out across many machines. It is expressible in Java Streams API without too much work. We'll start from the point where we already prepared a Stream&lt;Entry&lt;Long, String&gt;&gt; docWords: a stream of all the words found in all the documents. We use Map.Entry as a holder of a pair of values (a 2-tuple) and here we have a pair of Long docId and String word. We also already know the number of all documents and have a double logDocCount, the logarithm of the document count, ready. Calculating TF is very easy, just count the number of occurrences of each distinct pair and save the result in a Map&lt;Entry&lt;Long, String&gt;, Long&gt;: \/\/ TF map: (docId, word) -&gt; count final Map&lt;Entry&lt;Long, String&gt;, Long&gt; tfMap = docWords .parallel() .collect(groupingBy(identity(), counting())); And now we build the inverted index. We start from tfMap, group by word, and the list under each word already matches our final product: the list of all the documents containing the word. We finish off by applying a transformation to the list: currently it's just the raw entries from the tf map, but we need pairs (docId, tfIDfScore). invertedIndex = tfMap .entrySet() \/\/ set of ((docId, word), count) .parallelStream() .collect(groupingBy( e -&gt; e.getKey().getValue(), collectingAndThen( toList(), entries -&gt; { double idf = logDocCount - Math.log(entries.size()); return entries.stream() .map(e -&gt; tfidfEntry(e, idf)) .collect(toList()); } ) )); \/\/ ((docId, word), count) -&gt; (docId, tfIdf) private static Entry&lt;Long, Double&gt; tfidfEntry( Entry&lt;Entry&lt;Long, String&gt;, Long&gt; tfEntry, Double idf ) { final Long tf = tfEntry.getValue(); return entry(tfEntry.getKey().getKey(), tf * idf); } The search function can be implemented with another Streams expression, which you can review in the SearchGui class. You can also run the TfIdfJdkStreams class and take the inverted index for a spin, making actual searches. There is one last concept in this model that we haven't mentioned yet: the stopword set. It contains those words that are known in advance to be common enough to occur in every document. Without treatment, these words are the worst case for the inverted index: the document list under each such word is the longest possible, and the score of all documents is zero due to zero IDF. They raise the index's memory footprint without providing any value. The cure is to prepare a file, stopwords.txt, which is read in advance into a Set&lt;String&gt; and used to filter out the words in the tokenization phase. The same set is used to cross out words from the user's search phrase, as if they weren't entered.","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Building_Inverted_Index_with_Java_Streams.html"},{"title":"Translating to Jet DAG","text":"The code given so far is a great option as long as your dataset is small enough to not require scaling out to a cluster. There are some caveats due to the details of how the JDK's Fork\/Join engine parallelizes the work, but when done right it will perform very well. The concerns of scaling out, however, have a significant impact on the shape of the computation. While in single-node concurrent computing a major challenge is sharing mutable data, in multi-node distributed computing the major challenge is simply sharing --- of any kind.","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/index.html"},{"title":"Partitioning and Merging Partial Results","text":"We want as much isolation as possible between the computing nodes: each one should ideally read its own slice of the dataset, process it locally, and only send the aggregated data across the network for combining into the final result. The major point of leverage is the concept of partitioning: the ability to tell for a data item which processing unit it belongs to, just by looking at the item. This means that nodes need no coordination to sort this out. To be effective, partitioning must be applied right at the source: each node must read a non-overlapping slice of the data. In our case we achieve it by putting the filenames into an IMap in order to exploit Hazelcast's built-in partitioning. The map reader vertex will read just the locally-stored partitions on each cluster member. The next major point where partitioning happens is on any edge going into a vertex that groups the items by key: all items with the same key must be routed to the same processing unit. (As a reminder, a single vertex is implemented by many processing units distributed across the cluster.) There are two variations here: we can partition the data but let it stay within the same machine (each member will have its own processor for any given key), or we can partition and distribute it, so that for each key there is only one processing unit in the whole cluster that gets all the items regardless of where they were emitted. In a well-designed DAG the data will first be grouped and aggregated locally, and then only the aggregated partial results will be sent over a distributed edge, to be combined key-by-key into the complete result. In the case of TF-IDF, the TF part is calculated in the context of a single document. Since the data source is partitioned by document, we can calculate TF locally without sharing anything across the cluster. Then, to get the complete TF-IDF, we have to send just one item per distinct document-word combination over the network to the processing unit that will group them by word.","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/Partitioning_and_Merging_Partial_Results.html"},{"title":"DAG Vertices - Computation Steps","text":"The general outline of most DAGs is a cascade of vertices starting from a source and ending in a sink. Each grouping operation will typically be done in its own vertex. We have two such operations: the first one prepares the TF map and the second one builds the inverted index. Flatmap-like operations (this also encompasses map and filter as special cases) are simple to distribute because they operate on each item independently. Such an operation can be attached to the work of an existing vertex; however concerns like blocking I\/O and load balancing encourage the use of dedicated flatmapping vertices. In our case we'll have one flatmapping vertex that transforms a filename into a stream of the file's lines and another one that tokenizes a line into a stream of its words. The file-reading vertex will have to use non-cooperative processors due to the blocking I\/O and while a processor is blocking to read more lines, the tokenizing processors can run at full speed, processing the lines already read. This is the outline of the DAG's &quot;backbone&quot; --- the main cascade where the data flows from the source to the sink: The data source is a Hazelcast IMap which holds a mapping from document ID to its filename. The source vertex will emit all the map's entries, but only a subset on each cluster member. doc-lines opens each file named by the map entry and emits all its lines in the (docId, line) format. tokenize transforms each line into a sequence of its words, again paired with the document ID: (docId, word). tf builds a set of all distinct tuples and maintains the count of each tuple's occurrences (its TF score). tf-idf takes that set, groups the tuples by word, and calculates the TF-IDF scores. It emits the results to the sink, which saves them to a distributed IMap. To this cascade we add a stopword-source which reads the stopwords file, parses it into a HashSet, and sends the whole set as a single item to the tokenize vertex. We also add a vertex that takes the data from doc-source and simply counts its items; this is the total document count used in the TF-IDF formula. It feeds this result into tf-idf. We end up with this DAG: ------------ ----------------- | doc-source | | stopword-source | ------------ ----------------- 0 \/ \\ 1 | \/ (docId, docName) | \/ \\ | \/ V (set-of-stopwords) (docId, docName) ----------- | | | doc-lines | | | ----------- | | | | | (docId, line) | ----------- | | | doc-count | V 1 | ----------- ---------- 0 | | | tokenize | &lt;------\/ | ---------- | | (count) (docId, word) | | | V | ---- | | tf | | ---- | | | ((docId, word), count) | | | 0 -------- 1 | \\--&gt; | tf-idf | &lt;---\/ -------- | (word, list(docId, tfidf-score) | V ------ | sink | ------","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/DAG_Vertices_-_Computation_Steps.html"},{"title":"DAG Edges - Data Routing Concerns","text":"Let us now focus on the data routing aspect. doc-lines is a flatmapping vertex so the edge towards it doesn't need partitioning. Also, since the vertex does file I\/O, we usually won't profit from parallelization. We set its localParallelism to 1, so all the items (filenames) emitted from the source go to the same file I\/O processor. dag.edge(from(docSource, 1).to(docLines.localParallelism(1))); tokenize is another flatmapping vertex so it doesn't need partitioning, either. However, since this is a purely computational vertex, there's exploitable parallelism. The combination of a &quot;plain&quot; edge and a vertex with a higher localParallelism results in a round-robin dissemination of items from doc-lines to all tokenize processors: each item is sent to one processor, but a different one each time. dag.edge(from(docLines).to(tokenize, 1)); tf groups the items; therefore the edge towards it must be partitioned and the partitioning key must match the grouping key. In this case it's the item as a whole. The edge can be local because the data is already naturally partitioned by document such that for any given docId, all tuples involving it will occur on the same cluster member. dag.edge(between(tokenize, tf).partitioned(wholeItem(), HASH_CODE)); tf-idf groups the items by word alone. Since the same word can occur on any member, we need a distributed partitioned edge from tf to tf-idf. This will ensure that for any given word, there is a total of one processor in the whole cluster that receives tuples involving it. Distributed.Function&lt;Entry&lt;Entry&lt;?, String&gt;, ?&gt;, String&gt; byWord = item -&gt; item.getKey().getValue(); dag.edge(from(tf).to(tfidf, 1).distributed().partitioned(byWord, HASH_CODE)); The edge from stopword-source to tokenize transfers a single item, but it must deliver it to all tokenize processors. In our example, the same stopwords file is accessible on all members and the stopword-source processor reads it on each member independently. Therefore a local broadcast edge is the correct choice: its effect will be to publish the reference to the local HashSet to all tokenize processors. This edge must have a raised priority because tokenize cannot do its job until it has received the stopwords. dag.edge(between(stopwordSource.localParallelism(1), tokenize) .broadcast().priority(-1)) doc-count receives data from a distributed, partitioned data source but needs to see all the items to come up with the total count. The choice here is to set its localParallelism to one and configure its inbound edge as distributed broadcast: each processor will observe all the items, emitted on any member. It can then deliver its count over a local broadcast, high-priority edge to all the local tf-idf processors. dag.edge(between(docSource.localParallelism(1), docCount.localParallelism(1)) .distributed().broadcast()); .edge(between(docCount, tfidf).broadcast().priority(-1))","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/DAG_Edges_-_Data_Routing_Concerns.html"},{"title":"Partitioning Strategy","text":"The concern of coming up with a partition ID for an item has two aspects: extract the partitioning key from the item; calculate the partition ID from the key. The first point is covered by the key extractor function and the second by the Partitioner type. In most cases the choice of partitioner boils down to two types provided out of the box: Default Hazelcast partitioner: safe but slower; Object.hashCode()-based partitioner: typically faster, but not safe in general. The trouble with Object.hashCode() is that its contract is only concerned with instances that live within the same JVM. It says nothing about the correspondence of hash codes on two separate JVM processes, but for distributed edges it is essential that the hashcode of the deserialized object stays the same as the original. Some clasess, like String or Integer, specify exactly how they calculate the hashcode; these types are safe to be partitioned by hashcode. When these guarantees don't exist, the default partitioner can be used. It will serialize the object and use Hazelcast's standard MurmurHash3 algorithm to get the partition ID. Both aspects of partitioning are specified as arguments to the edge's partitioned() method. This example specifies default Hazelcast partitioner: edge.partitioned(wholeItem()); and this one specifies the Object.hashCode() strategy: edge.partitioned(wholeItem(), HASH_CODE));","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/Partitioning_Strategy.html"},{"title":"Implementing the Vertex Computation","text":"The source vertex reads a Hazelcast IMap so we just use the processor provided in Jet: dag.newVertex(&quot;doc-source&quot;, Processors.readMap(DOCID_NAME)); The stopwords-producing vertex has a custom processor: dag.newVertex(&quot;stopword-source&quot;, StopwordsP::new); The processor's implementation is quite simple: private static class StopwordsP extends AbstractProcessor { @Override public boolean complete() { emit(docLines(&quot;stopwords.txt&quot;).collect(toSet())); return true; } } It emits a single item: the HashSet built directly from the stream of a text file's lines. The doc-count processor can again be built from the primitives provided in the Jet's library: dag.newVertex(&quot;doc-count&quot;, Processors.accumulate(() -&gt; 0L, (count, x) -&gt; count + 1)); The doc-lines processor is more of a mouthful, but still built from existing primitives: dag.newVertex(&quot;doc-lines&quot;, nonCooperative( Processors.flatMap((Entry&lt;Long, String&gt; e) -&gt; traverseStream(docLines(&quot;books\/&quot; + e.getValue()) .map(line -&gt; entry(e.getKey(), line)))))); Let's break down this expression... Processors.flatMap returns a standard processor that emits an arbitrary number of items for each received item. The user supplies a function of the shape inputItem -&gt; Traverser(outputItems) and the processor takes care of all the logic required to cooperatively emit those items while respecting the output buffer limits. This is the user-supplied expression evaluated for each incoming item: traverseStream(docLines(&quot;books\/&quot; + e.getValue()) .map(line -&gt; entry(e.getKey(), line)))) traverseStream converts a java.util.Stream to Traverser so the inner part builds the stream: docLines() simply returns Files.lines(Paths.get(TfIdf.class.getResource(name).toURI())) and then the mapping stage is applied, which creates a pair (docId, line). Finally, the whole processor expression is wrapped into a call of nonCooperative() which will declare the processor non-cooperative, as required by the fact that it does blocking file I\/O. tokenizer is another custom vertex: dag.newVertex(&quot;tokenize&quot;, TokenizeP::new); private static class TokenizeP extends AbstractProcessor { private Set&lt;String&gt; stopwords; private final FlatMapper&lt;Entry&lt;Long, String&gt;, Entry&lt;Long, String&gt;&gt; flatMapper = flatMapper(e -&gt; traverseStream( Arrays.stream(DELIMITER.split(e.getValue())) .filter(word -&gt; !stopwords.contains(word)) .map(word -&gt; entry(e.getKey(), word)))); @Override protected boolean tryProcess0(@Nonnull Object item) { stopwords = (Set&lt;String&gt;) item; return true; } @Override protected boolean tryProcess1(@Nonnull Object item) { return flatMapper.tryProcess((Entry&lt;Long, String&gt;) item); } } This is a processor that must deal with two different inbound edges. It receives the stopword set over edge 0 and then it does a flatmapping operation on edge 1. The logic presented here uses the same approach as the implementation of the provided Processors.flatMap() processor: there is a single instance of FlatMapper that holds the business logic of the transformation, and the tryProcess1() callback method directly delegates into it. If FlatMapper is done emitting the previous items, it will accept the new item, apply the user-provided transformation, and start emitting the output items. If the buffer state prevents it from emitting all the pending items, it will return false, which will make the framework call the same tryProcess1 method later, with the same input item. Let's show the code that creates the tokenize's two inbound edges: dag.edge(between(stopwordSource, tokenize).broadcast().priority(-1)) .edge(from(docLines).to(tokenize, 1)); Especially note the .priority(-1) part: this ensures that there will be no attempt to deliver any data coming from docLines before all the data from stopwordSource is already delivered. The processor would fail if it had to tokenize a line before it has its stopword set in place. tf is another simple vertex, built purely from the provided primitives: dag.newVertex(&quot;tf&quot;, groupAndAccumulate(() -&gt; 0L, (count, x) -&gt; count + 1)); tf-idf is the most complex processor: dag.newVertex(&quot;tf-idf&quot;, TfIdfP::new); private static class TfIdfP extends AbstractProcessor { private double logDocCount; private final Map&lt;String, List&lt;Entry&lt;Long, Double&gt;&gt;&gt; wordDocTf = new HashMap&lt;&gt;(); private final Traverser&lt;Entry&lt;String, List&lt;Entry&lt;Long, Double&gt;&gt;&gt;&gt; invertedIndexTraverser = lazy(() -&gt; traverseIterable(wordDocTf.entrySet()).map(this::toInvertedIndexEntry)); @Override protected boolean tryProcess0(@Nonnull Object item) throws Exception { logDocCount = Math.log((Long) item); return true; } @Override protected boolean tryProcess1(@Nonnull Object item) throws Exception { final Entry&lt;Entry&lt;Long, String&gt;, Long&gt; e = (Entry&lt;Entry&lt;Long, String&gt;, Long&gt;) item; final long docId = e.getKey().getKey(); final String word = e.getKey().getValue(); final long tf = e.getValue(); wordDocTf.computeIfAbsent(word, w -&gt; new ArrayList&lt;&gt;()) .add(entry(docId, (double) tf)); return true; } @Override public boolean complete() { return emitCooperatively(invertedIndexTraverser); } private Entry&lt;String, List&lt;Entry&lt;Long, Double&gt;&gt;&gt; toInvertedIndexEntry( Entry&lt;String, List&lt;Entry&lt;Long, Double&gt;&gt;&gt; wordDocTf ) { final String word = wordDocTf.getKey(); final List&lt;Entry&lt;Long, Double&gt;&gt; docidTfs = wordDocTf.getValue(); return entry(word, docScores(docidTfs)); } private List&lt;Entry&lt;Long, Double&gt;&gt; docScores(List&lt;Entry&lt;Long, Double&gt;&gt; docidTfs) { final double logDf = Math.log(docidTfs.size()); return docidTfs.stream() .map(tfe -&gt; tfidfEntry(logDf, tfe)) .collect(toList()); } private Entry&lt;Long, Double&gt; tfidfEntry(double logDf, Entry&lt;Long, Double&gt; docidTf) { final Long docId = docidTf.getKey(); final double tf = docidTf.getValue(); final double idf = logDocCount - logDf; return entry(docId, tf * idf); } } This is quite a lot of code, but each of the three pieces is not too difficult to follow: tryProcess0() accepts a single item, the total document count. tryProcess1() performs a boilerplate groupBy operation, collecting a list of items under each key. complete() outputs the accumulated results, also applying the final transformation on each one: replacing the TF score with the final TF-IDF score. It relies on a lazy traverser, which holds a Supplier&lt;Traverser&gt; and will obtain the inner traverser from it the first time next() is called. This makes it very simple to write code that obtains a traverser from a map after it has been populated. Finally, our DAG is terminated by a sink vertex: dag.newVertex(&quot;sink&quot;, Processors.writeMap(INVERTED_INDEX));","tags":"","url":"Tutorial_-_Building_an_Inverted_TF-IDF_Index_with_Core_API\/Translating_to_Jet_DAG\/Implementing_the_Vertex_Computation.html"},{"title":"Understanding Configuration","text":"You can configure Hazelcast Jet either programmatically or declaratively (XML).","tags":"","url":"Understanding_Configuration\/index.html"},{"title":"Configuring Programatically","text":"Programmatic configuration is the simplest way to configure Jet. For example, the following will configure Jet to use only two threads for cooperative execution: JetConfig config = new JetConfig(); config.getInstanceConfig().setCooperativeThreadCount(2); JetInstance jet = Jet.newJetInstance(config); Any XML configuration files that might be present will be ignored when programmatic configuration is used.","tags":"","url":"Understanding_Configuration\/Configuring_Programatically.html"},{"title":"Configuring Declaratively","text":"It is also possible to configure Jet through XML files when a JetInstance is created without any explicit JetConfig file. Jet will look for a configuration file in the following order: Check the system property hazelcast.jet.config. If the value is set, and starts with classpath:, then it will be treated as a classpath resource. Otherwise, it will be treated as a file reference. Check for the presence of hazelcast-jet.xml in the working directory. Check for the presence of hazelcast-jet.xml in the classpath. If all the above checks fail, then the default XML configuration will be loaded. An example configuration looks like the following: &lt;hazelcast-jet xsi:schemaLocation=&quot;http:\/\/www.hazelcast.com\/schema\/jet-config hazelcast-jet-config-0.3.xsd&quot; xmlns=&quot;http:\/\/www.hazelcast.com\/schema\/jet-config&quot; xmlns:xsi=&quot;http:\/\/www.w3.org\/2001\/XMLSchema-instance&quot;&gt; &lt;instance&gt; &lt;!-- number of threads to use for DAG execution --&gt; &lt;cooperative-thread-count&gt;8&lt;\/cooperative-thread-count&gt; &lt;!-- frequency of flow control packets, in milliseconds --&gt; &lt;flow-control-period&gt;100&lt;\/flow-control-period&gt; &lt;!-- working directory to use for placing temporary files --&gt; &lt;temp-dir&gt;\/var\/tmp\/jet&lt;\/temp-dir&gt; &lt;\/instance&gt; &lt;properties&gt; &lt;property name=&quot;custom.property&quot;&gt;custom property&lt;\/property&gt; &lt;\/properties&gt; &lt;edge-defaults&gt; &lt;!-- number of available slots for each concurrent queue between two vertices --&gt; &lt;queue-size&gt;1024&lt;\/queue-size&gt; &lt;!-- number of slots in each outbox's bucket --&gt; &lt;outbox-capacity&gt;2048&lt;\/outbox-capacity&gt; &lt;!-- maximum packet size in bytes, only applies to distributed edges --&gt; &lt;packet-size-limit&gt;16384&lt;\/packet-size-limit&gt; &lt;!-- target receive window size multiplier, only applies to distributed edges --&gt; &lt;receive-window-multiplier&gt;3&lt;\/receive-window-multiplier&gt; &lt;\/edge-defaults&gt; &lt;!-- custom properties which can be read within a ProcessorSupplier --&gt; &lt;\/hazelcast-jet&gt; The following table lists the configuration elements for Hazelcast Jet: Name Description Default Value Cooperative Thread Count Maximum number of cooperative threads to be used for execution of jobs. Runtime.getRuntime().availableProcessors() Temp Directory Directory where temporary files will be placed, such as JAR files submitted by clients. Jet will create a temp directory, which will be deleted on exit. Flow Control Period While executing a Jet job there is the issue of regulating the rate at which one member of the cluster sends data to another member. The receiver will regularly report to each sender how much more data it is allowed to send over a given DAG edge. This option sets the length (in milliseconds) of the interval between flow-control packets. 100ms Edge Defaults The default values to be used for all edges. Please see the Tuning Edges section.","tags":"","url":"Understanding_Configuration\/Configuring_Declaratively.html"},{"title":"Configuring Underlying Hazelcast Instance","text":"Each Jet member or client, will have a respective underlying Hazelcast member or client. Please refer to the Hazelcast Reference Manual for specific configuration options for Hazelcast IMDG. Programmatic The underlying Hazelcast IMDG member can be configured as follows: JetConfig jetConfig = new JetConfig(); jetConfig.getHazelcastConfig().getGroupConfig().setName(&quot;test&quot;); JetInstance jet = Jet.newJetInstance(jetConfig); The underlying Hazelcast IMDG client can be configured as follows: ClientConfig clientConfig = new ClientConfig(); clientConfig.getGroupConfig().setName(&quot;test&quot;); JetInstance jet = Jet.newJetClient(clientConfig); Declarative The underlying Hazelcast IMDG configuration can also be updated declaratively. Please refer to the Hazelcast Reference Manual for information on how to do this.","tags":"","url":"Understanding_Configuration\/Configuring_Underlying_Hazelcast_Instance.html"},{"title":"Implementing Custom Sources and Sinks","text":"Hazelcast Jet provides a flexible API that makes it easy to implement your own custom sources and sinks. Both sources and sinks are implemented using the same API as the rest of the Processors. In this chapter we will work through some examples as a guide to how you can connect Jet with your own data sources.","tags":"","url":"Implementing_Custom_Sources_and_Sinks\/index.html"},{"title":"Sources","text":"One of the main concerns when writing custom sources is that the source is typically distributed across multiple machines and partitions, and the work needs to be distributed across multiple members and processors. Jet provides a flexible ProcessorMetaSupplier and ProcessorSupplier API which can be used to control how a source is distributed across the network. The procedure for generating Processor instances is as follows: The ProcessorMetaSupplier for the Vertex is serialized and sent to the coordinating member. The coordinator calls ProcessorMetaSupplier.get() once for each member in the cluster and a ProcessorSupplier is created for each member. The ProcessorSupplier for each member is serialized and sent to that member. Each member will call their own ProcessorSupplier with the correct count parameter, which corresponds to the localParallelism setting of that vertex.","tags":"","url":"Implementing_Custom_Sources_and_Sinks\/Sources.html"},{"title":"Example - Distributed Integer Generator","text":"Let's say we want to write a simple source that will generate numbers from 0 to 1,000,000 (exclusive). It is trivial to write a single Processor which can do this using java.util.stream and Traverser. class GenerateNumbersP extends AbstractProcessor { private final Traverser&lt;Integer&gt; traverser; GenerateNumbersP(int upperBound) { traverser = Traversers.traverseStream(IntStream.range(0, upperBound).boxed()); } @Override public boolean complete() { return emitFromTraverser(traverser); } } We will also add a simple logging processor so we can see what values are generated: class LogInputP extends AbstractProcessor { LogInputP() { setCooperative(false); } @Override protected boolean tryProcess(int ordinal, @Nonnull Object item) { System.out.println(&quot;Received number: &quot; + item); emit(item); return true; } } Now we can build our DAG and execute it: JetInstance jet = Jet.newJetInstance(); int upperBound = 10; DAG dag = new DAG(); Vertex generateNumbers = dag.newVertex(&quot;generate-numbers&quot;, () -&gt; new GenerateNumbersP(upperBound)); Vertex logInput = dag.newVertex(&quot;log-input&quot;, LogInputP::new); dag.edge(Edge.between(generateNumbers, logInput)); try { jet.newJob(dag).execute().get(); } finally { Jet.shutdownAll(); } When you run this code, you will see the output as below: Received number: 4 Received number: 0 Received number: 3 Received number: 2 Received number: 2 Received number: 2 Since we are using the default parallelism setting on this vertex, several instances of the source processor were created, all of which generated the same sequence of values. Generally we'll want the ability to parallelize the source vertex, so we have to make each processor emit only a slice of the total data set. So far we've used the simplest approach to creating processors: a Supplier&lt;Processor&gt; function that keeps returning equal instances of processors. Now we'll step up to Jet's custom interface that gives us the ability to provide a list of separately configured processors: ProcessorSupplier and its method get(int processorCount). First we must decide on a partitioning policy: what subset will each processor emit. In our simple example we can use a simple policy: we'll label each processor with its index in the list and have it emit only those numbers n that satisfy n % processorCount == processorIndex. Let's write a new constructor for our processor which implements this partitioning logic: GenerateNumbersP(int upperBound, int processorCount, int processorIndex) { traverser = Traversers.traverseStream( IntStream.range(0, upperBound) .filter(n -&gt; n % processorCount == processorIndex) .boxed()); } Given this preparation, implementing ProcessorSupplier is trivial: class GenerateNumbersPSupplier implements ProcessorSupplier { private final int upperBound; GenerateNumbersPSupplier(int upperBound) { this.upperBound = upperBound; } @Override @Nonnull public List&lt;? extends Processor&gt; get(int processorCount) { return IntStream.range(0, processorCount) .mapToObj(index -&gt; new GenerateNumbersP(upperBound, processorCount, index)) .collect(Collectors.toList()); } } Let's use the custom processor supplier in our DAG-building code: DAG dag = new DAG(); Vertex generateNumbers = dag.newVertex(&quot;generate-numbers&quot;, new GenerateNumbersPSupplier(10)); Vertex logInput = dag.newVertex(&quot;log-input&quot;, LogInputP::new); dag.edge(Edge.between(generateNumbers, logInput)); Now we can re-run our example and see that indeed each number occurs only once. However, note that we are still working with a single-member Jet cluster; let's see what happens when we add another member: JetInstance jet = Jet.newJetInstance(); Jet.newJetInstance(); DAG dag = new DAG(); ... Running after this change we'll see that both members are generating the same set of numbers. This is because ProcessorSupplier is instantiated independently for each member and asked for the same number of processors, resulting in identical processors on all members. We have to solve the same problem as we just did, but at the higher level of cluster-wide parallelism. For that we'll need the ProcessorMetaSupplier: an interface which acts as a factory of ProcessorSuppliers, one for each cluster member. Under the hood it is actually always the meta-supplier that's created by the DAG-building code; the above examples are just implicit about it for the sake of convenience. They result in a simple meta-supplier that reuses the provided suppliers everywhere. The meta-supplier is a bit trickier to implement because its method takes a list of Jet member addresses instead of a simple count, and the return value is a function from address to ProcessorSupplier. In our case we'll treat the address as just an opaque ID and we'll build a map from address to a properly configured ProcessorSupplier. Then we can simply return map::get as our function. class GenerateNumbersPMetaSupplier implements ProcessorMetaSupplier { private final int upperBound; private transient int totalParallelism; private transient int localParallelism; GenerateNumbersPMetaSupplier(int upperBound) { this.upperBound = upperBound; } @Override public void init(@Nonnull Context context) { totalParallelism = context.totalParallelism(); localParallelism = context.localParallelism(); } @Override @Nonnull public Function&lt;Address, ProcessorSupplier&gt; get(@Nonnull List&lt;Address&gt; addresses) { Map&lt;Address, ProcessorSupplier&gt; map = new HashMap&lt;&gt;(); for (int i = 0; i &lt; addresses.size(); i++) { \/\/ We'll calculate the global index of each processor in the cluster: int globalIndexBase = localParallelism * i; \/\/ Capture the value of the transient field for the lambdas below: int divisor = totalParallelism; \/\/ processorCount will be equal to localParallelism: ProcessorSupplier supplier = processorCount -&gt; range(globalIndexBase, globalIndexBase + processorCount) .mapToObj(globalIndex -&gt; new GenerateNumbersP(upperBound, divisor, globalIndex) ).collect(toList()); map.put(addresses.get(i), supplier); } return map::get; } } We change our DAG-building code to use the meta-supplier: DAG dag = new DAG(); Vertex generateNumbers = dag.newVertex(&quot;generate-numbers&quot;, new GenerateNumbersPMetaSupplier(upperBound)); Vertex logInput = dag.newVertex(&quot;log-input&quot;, LogInputP::new); dag.edge(Edge.between(generateNumbers, logInput)); After re-running with two Jet members, we should once again see each number generated just once.","tags":"","url":"Implementing_Custom_Sources_and_Sinks\/Example_-_Distributed_Integer_Generator.html"},{"title":"Sinks","text":"Like a source, a sink is just another kind of processor. It accepts items from the inbox and pushes them into some system external to the Jet job (Hazelcast IMap, files, databases, distributed queues, etc.). A simple way to implement it is to extend AbstractProcessor and override tryProcess, which deals with items one at a time. However, sink processors must often explicitly deal with batching. In this case directly implementing Processor is better because its process() method gets the entire Inbox which can be drained to a buffer and flushed out.","tags":"","url":"Implementing_Custom_Sources_and_Sinks\/Sinks.html"},{"title":"Example - File Writer","text":"In this example we'll implement a vertex that writes the received items to files. To avoid contention and conflicts, each processor must write to its own file. Since we'll be using a BufferedWriter which takes care of the buffering\/batching concern, we can use the simpler approach of extending AbstractProcessor: class WriteFileP extends AbstractProcessor implements Closeable { private final String path; private transient BufferedWriter writer; WriteFileP(String path) { setCooperative(false); this.path = path; } @Override protected void init(@Nonnull Context context) throws Exception { Path path = Paths.get(this.path, context.jetInstance().getName() + '-' + context.globalProcessorIndex()); writer = Files.newBufferedWriter(path, StandardCharsets.UTF_8); } @Override protected boolean tryProcess(int ordinal, Object item) throws Exception { writer.append(item.toString()); writer.newLine(); return true; } @Override public void close() throws IOException { if (writer != null) { writer.close(); } } } Some comments: The constructor declares the processor non-cooperative because it will perform blocking IO operations. init() method finds a unique filename for each processor by relying on the information reachable from the Context object. Note the careful implementation of close(): it first checks if writer is null, which can happen if newBufferedWriter() fails in init(). This would make init() fail as well, which would make the whole job fail and then our ProcessorSupplier would call close() to clean up. Cleaning up on completion\/failure is actually the only concern that we need ProcessorSupplier for: the other typical concern, specializing processors to achieve data partitioning, was achieved directly from the processor's code. This is the supplier's code: class WriteFilePSupplier implements ProcessorSupplier { private final String path; private transient List&lt;WriteFileP&gt; processors; WriteFilePSupplier(String path) { this.path = path; } @Override public void init(@Nonnull Context context) { File homeDir = new File(path); boolean success = homeDir.isDirectory() || homeDir.mkdirs(); if (!success) { throw new JetException(&quot;Failed to create &quot; + homeDir); } } @Override @Nonnull public List&lt;WriteFileP&gt; get(int count) { processors = Stream.generate(() -&gt; new WriteFileP(path)) .limit(count) .collect(Collectors.toList()); return processors; } @Override public void complete(Throwable error) { for (WriteFileP p : processors) { try { p.close(); } catch (IOException e) { throw new JetException(e); } } } }","tags":"","url":"Implementing_Custom_Sources_and_Sinks\/Example_-_File_Writer.html"},{"title":"java-util-stream Support for Hazelcast IMDG","text":"Hazelcast Jet adds distributed java.util.stream support for Hazelcast IMap and IList data structures. For extensive information about java.util.stream API please refer to the official javadocs.","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/index.html"},{"title":"Simple Example","text":"JetInstance jet = Jet.newJetInstance(); IStreamMap&lt;String, Integer&gt; map = jet.getMap(&quot;latitudes&quot;); map.put(&quot;London&quot;, 51); map.put(&quot;Paris&quot;, 48); map.put(&quot;NYC&quot;, 40); map.put(&quot;Sydney&quot;, -34); map.put(&quot;Sao Paulo&quot;, -23); map.put(&quot;Jakarta&quot;, -6); map.stream().filter(e -&gt; e.getValue() &lt; 0).forEach(System.out::println);","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/Simple_Example.html"},{"title":"Serializable Lambda Functions","text":"By default, the functional interfaces which were added to java.util.function are not serializable. In a distributed system, the defined lambdas need to be serialized and sent to the other members. Jet includes the serializable version of all the interfaces found in the java.util.function which can be accessed using the com.hazelcast.jet.stream.Distributed class.","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/Serializable_Lambda_Functions.html"},{"title":"Distributed Collectors","text":"Like with the functional interfaces, Jet also includes the distributed versions of the classes found in java.util.stream.Collectors. These can be reached via com.hazelcast.jet.stream.DistributedCollectors. However, keep in mind that collectors such as toMap(), toCollection(), toList(), and toArray() create a local data structure and load all the results into it. This works fine with the regular JDK streams, where everything is local, but usually fails badly in the context of a distributed computing job. For, example the following innocent-looking code can easily cause out-of-memory errors because the whole distributed map will need to be held in memory at a single place: \/\/ get a distributed map with 5GB per node on a 10 node cluster IStreamMap&lt;String, String&gt; map = jet.getMap(&quot;large_map&quot;); \/\/ now try to build a HashMap of 50GB Map&lt;String, String&gt; result = map.stream() .map(e -&gt; e.getKey() + e.getValue()) .collect(toMap(v -&gt; v, v -&gt; v)); This is why Jet distinguishes between the standard java.util.stream collectors and the Jet-specific Reducers. A Reducer puts the data into a distributed data structure and knows how to leverage its native partitioning scheme to optimize the access pattern across the cluster. These are some of the Reducer implementations provided in Jet: toIMap(): writes the data to a new Hazelcast IMap. groupingByToIMap(): performs a grouping operation and then writes the results to a Hazelcast IMap. This uses a more efficient implementation than the standard groupingBy() collector and can make use of partitioning. toIList(): writes the data to a new Hazelcast IList. A distributed data structure is cluster-managed, therefore you can't just create one and forget about it; it will live on until you explicitly destroy it. That means it's inappropriate to use as a part of a data item inside a larger collection, a further consequence being that a Reducer is inappropriate as a downstream collector; that's where the JDK-standard collectors make sense.","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/Distributed_Collectors.html"},{"title":"Word Count","text":"The word count example that was described in the Hazelcast Jet 101 chapter can be rewritten using the java.util.stream API as follows: IMap&lt;String, Long&gt; counts = lines .stream() .flatMap(m -&gt; Stream.of(PATTERN.split(m.getValue().toLowerCase()))) .collect(DistributedCollectors.toIMap(w -&gt; w, w -&gt; 1L, (left, right) -&gt; left + right));","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/Word_Count.html"},{"title":"Implementation Notes","text":"Jet's java.util.stream implementation will automatically convert a stream into a DAG when one of the terminal methods are called. The DAG creation is done lazily, and only if a terminal method is called. The following DAG will be compiled as follows: IStreamMap&lt;String, Integer&gt; ints = jet.getMap(&quot;ints&quot;); int result = ints.stream().map(Entry::getValue) .reduce(0, (l, r) -&gt; l + r);","tags":"","url":"java-util-stream_Support_for_Hazelcast_IMDG\/Implementation_Notes.html"},{"title":"Additional Modules","text":"Hazelcast Jet comes with modules that can be used to connect to additional data sources and sinks.","tags":"","url":"Additional_Modules\/index.html"},{"title":"hazelcast-jet-hadoop","text":"The hazelcast-jet-hadoop module provides read and write capabilities to Apache Hadoop. The ReadHdfsP and WriteHdfsP classes provide source and sink processors which can be used for reading and writing, respectively. The processors take a JobConf as parameters which can be used to specify the InputFormat, OutputFormat and their respective paths. Example: JobConf jobConf = new JobConf(); jobConf.setInputFormat(TextInputFormat.class); jobConf.setOutputFormat(TextOutputFormat.class); TextInputFormat.addInputPath(jobConf, inputPath); TextOutputFormat.setOutputPath(jobConf, outputPath); Vertex source = dag.newVertex(&quot;source&quot;, ReadHdfsP.readHdfs(jobConf)); Vertex sink = dag.newVertex(&quot;sink&quot;, WriteHdfsP.writeHdfs(jobConf)); ... See the HDFS code sample for a fully worked example.","tags":"","url":"Additional_Modules\/hazelcast-jet-hadoop\/index.html"},{"title":"ReadHdfsP","text":"ReadHdfsP is used to read items from one or more HDFS files. The input is split according to the given InputFormat and read in parallel across all processor instances. ReadHdfsP by default emits items of type Map.Entry&lt;K,V&gt;, where K and V are the parameters for the given InputFormat. It is possible to transform the records using an optional mapper parameter. For example, in the above example the output record type of TextInputFormat is Map.Entry&lt;LongWritable, Text&gt;. LongWritable represents the line number and Text the contents of the line. If you do not care about line numbers, and want your output as a plain String, you can do as follows: Vertex source = dag.newVertex(&quot;source&quot;, readHdfs(jobConf, (k, v) -&gt; v.toString())); With this change, ReadHdfsP will emit items of type String instead. Cluster Co-location The Jet cluster should be run on the same nodes as the HDFS nodes for best read performance. If this is the case, each processor instance will try to read as much local data as possible. A heuristic algorithm is used to assign replicated blocks across the cluster to ensure a well-balanced work distribution between processor instances for maximum performance.","tags":"","url":"Additional_Modules\/hazelcast-jet-hadoop\/ReadHdfsP.html"},{"title":"WriteHdfsP","text":"WriteHdfsP expects items of type Map.Entry&lt;K,V&gt; and writes the key and value parts to HDFS. Each processor instance creates a single file in the output path identified by the member ID and the processor ID. Unlike in MapReduce, the output files are not sorted by key and are written in the order they are received by the processor. A similar transformation to ReadHdfsP can be done for WriteHdfsP to map the incoming items into the required format. For example: Vertex sink = dag.newVertex(&quot;sink&quot;, WriteHdfsP.writeHdfs(jobConf, (String k) -&gt; new Text(k), (Long c) -&gt; new LongWritable(c))); This will transform the key and value to their Writable equivalents which can be required for certain OutputFormat implementations.","tags":"","url":"Additional_Modules\/hazelcast-jet-hadoop\/WriteHdfsP.html"},{"title":"Serialization of Writables","text":"Special care must be taken when serializing Writable items. The hazelcast-jet-hadoop module implements out-of-the-box serialization support for some of the primitive types including the following: BooleanWritable ByteWritable DoubleWritable FloatWritable IntWritable LongWritable Text Anything outside of these types falls back to a default implementation for all Writable types which writes the full class name and the fields per item. When deserializing, the class name is read first and the deserialized instance is created using the classloader and reflection. The explicitly registered types only write a single integer as a type id and do not use reflection for deserialization. To explicitly register your own Writable types for fast serialization, you can extend the provided WritableSerializerHook class and register the hook with Hazelcast.","tags":"","url":"Additional_Modules\/hazelcast-jet-hadoop\/Serialization_of_Writables.html"},{"title":"hazelcast-jet-kafka","text":"The hazelcast-jet-kafka module provides streaming read and write capabilities to Apache Kafka. The StreamKafkaP and WriteKafkaP classes provide source and sink processors which can be used for reading and writing, respectively. The processors take a list of properties given by Properties as a parameter which can be used to specify the group.id, bootstrap.servers, key\/value serializer\/deserializer and any other configuration parameters for Kafka. Example: Properties properties = new Properties(); properties.setProperty(&quot;group.id&quot;, &quot;group0&quot;); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;key.deserializer&quot;, StringDeserializer.class.getCanonicalName()); properties.setProperty(&quot;value.deserializer&quot;, IntegerDeserializer.class.getCanonicalName()); properties.setProperty(&quot;auto.offset.reset&quot;, &quot;earliest&quot;); Vertex source = dag.newVertex(&quot;source&quot;, StreamKafkaP.streamKafka(properties, &quot;topic1&quot;, &quot;topic2&quot;)); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); properties.setProperty(&quot;key.serializer&quot;, StringSerializer.class.getCanonicalName()); properties.setProperty(&quot;value.serializer&quot;, IntegerSerializer.class.getCanonicalName()); Vertex sink = dag.newVertex(&quot;sink&quot;, WriteKafkaP.writeKafka(&quot;topic1&quot;, properties)); For more details about configuring Kafka, see Apache Kafka Documentation.","tags":"","url":"Additional_Modules\/hazelcast-jet-kafka\/index.html"},{"title":"StreamKafkaP","text":"StreamKafkaP is used to consume items from one or more Kafka topics. It uses the Kafka consumer API and consumer groups to distribute partitions among processors where each partition is consumed by a single processor at any given time. The reader emits items of type Map.Entry&lt;K,V&gt; where the key and the value are deserialized using the key\/value deserializers configured in Kafka properties. Ideally, the total partition count in Kafka should be at least as large as the total parallelism in the Jet cluster (localParallelism*clusterSize) to make sure that each processor will get some partitions assigned to it. Internally, a KafkaConsumer is created per Processor instance using the supplied properties. All processors must be in the same consumer group supplied by the group.id property. It is required that the group.id is explicitly set by the user to a non-empty value. The supplied properties will be passed on to each KafkaConsumer instance. These processors are only terminated in case of an error or if the underlying job is cancelled. StreamKafkaP forces the enable.auto.commit property to be set to false, and commits the current offsets after they have been fully emitted.","tags":"","url":"Additional_Modules\/hazelcast-jet-kafka\/StreamKafkaP.html"},{"title":"WriteKafkaP","text":"WriteKafkaP is a processor which acts as a Kafka sink. It receives items of type Map.Entry and sends a ProducerRecord to the specified topic with key\/value parts which will be serialized according to the Kafka producer configuration. The key and value serializers set in the properties should be able to handle the keys and values received by the processor. Internally, a single KafkaProducer is created per node, which is shared among all Processor instances on that node. See the Kafka code sample for a fully worked example.","tags":"","url":"Additional_Modules\/hazelcast-jet-kafka\/WriteKafkaP.html"},{"title":"Best Practices","text":"This chapter describes the best practices to get the most out of Hazelcast Jet.","tags":"","url":"Best_Practices\/index.html"},{"title":"Jobs","text":"All the code and state needed for the Jet job must be declared in the classes that become a part of the job's definition through JobConfig.addClass() or addJar(). If you have a client connecting to your Jet cluster, the Jet job should never have to refer to ClientConfig. Create a separate DagBuilder class using the buildDag() method; this class should not have any references to the JobHelper class. You should have a careful control over the object graph which is submitted with the Jet job. Please be aware that inner classes\/lambdas may inadvertently capture their parent classes which will cause serialization errors.","tags":"","url":"Best_Practices\/Jobs.html"}]}